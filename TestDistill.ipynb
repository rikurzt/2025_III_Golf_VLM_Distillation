{
 "cells": [
  {
   "cell_type": "code",
   "id": "a718ff9cb8d75e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:14:11.537702Z",
     "start_time": "2025-09-04T10:14:10.564227Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:14:11.654950Z",
     "start_time": "2025-09-04T10:14:11.612489Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.get_device_name(0))",
   "id": "1d9da4fa10fddc2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX PRO 6000 Blackwell Workstation Edition\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:08:52.460723Z",
     "start_time": "2025-08-29T08:08:47.392352Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# System message for the assistant\n",
    "system_message = \"You are an expert product description writer for Amazon.\"\n",
    "\n",
    "# User prompt that combines the user query and the schema\n",
    "user_prompt = \"\"\"Create a Short Product description based on the provided <PRODUCT> and <CATEGORY> and image.\n",
    "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
    "\n",
    "<PRODUCT>\n",
    "{product}\n",
    "</PRODUCT>\n",
    "\n",
    "<CATEGORY>\n",
    "{category}\n",
    "</CATEGORY>\n",
    "\"\"\"\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_prompt.format(\n",
    "                            product=sample[\"Product Name\"],\n",
    "                            category=sample[\"Category\"],\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    image_inputs = []\n",
    "    # Iterate through each conversation\n",
    "    for msg in messages:\n",
    "        # Get content (ensure it's a list)\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        # Check each content element for images\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # Get the image and convert to RGB\n",
    "                if \"image\" in element:\n",
    "                    image = element[\"image\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                image_inputs.append(image.convert(\"RGB\"))\n",
    "    return image_inputs\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\", split=\"train\").select(range(50))\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[0][\"messages\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1345/1345 [00:00<00:00, 15393.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are an expert product description writer for Amazon.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"Create a Short Product description based on the provided <PRODUCT> and <CATEGORY> and image.\\nOnly return description. The description should be SEO optimized and for a better mobile search experience.\\n\\n<PRODUCT>\\nKurio Glow Smartwatch for Kids with Bluetooth, Apps, Camera & Games, Blue\\n</PRODUCT>\\n\\n<CATEGORY>\\nToys & Games | Kids' Electronics | Electronic Learning Toys\\n</CATEGORY>\\n\"}, {'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x7F90B55291B0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"Kurio Glow Smartwatch: Fun, Safe & Educational!  This kids' smartwatch boasts Bluetooth connectivity, built-in apps & games, and a camera – all in a vibrant blue design. Perfect for learning & play!  #kidssmartwatch #kidselectronics #educationaltoys #kurioglow\"}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fdb698e81c482b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T07:51:47.713851Z",
     "start_time": "2025-08-26T07:51:46.229858Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad42709ed66437d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:25:57.236903Z",
     "start_time": "2025-08-25T14:25:48.482749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-4b-pt\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    " #BitsAndBytesConfig int-4 config\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f720375af74532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:35:31.590557Z",
     "start_time": "2025-08-22T09:35:31.585185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): PytorchGELUTanh()\n",
      "                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
      "                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4759e01ae2c21f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:26:01.432835Z",
     "start_time": "2025-08-25T14:26:01.297822Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41b76590777ee4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:26:03.657426Z",
     "start_time": "2025-08-25T14:26:03.588941Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"model/gemma-test\",     # directory to save and repository id\n",
    "    num_train_epochs=1,                         # number of training epochs\n",
    "    per_device_train_batch_size=1,              # batch size per device during training\n",
    "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=5,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    ")\n",
    "args.remove_unused_columns = False # important for collator\n",
    "\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels.cpu()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a9d0ec7f3ad5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:26:06.661235Z",
     "start_time": "2025-08-25T14:26:05.898714Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd7bb0ff560772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:27:30.579001Z",
     "start_time": "2025-08-25T14:26:08.045676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.520800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0fe6e9e06041f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:00:22.548425Z",
     "start_time": "2025-08-11T12:00:22.540518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): PytorchGELUTanh()\n",
      "                (fc1): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=4304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (fc2): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4304, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): ModulesToSaveWrapper(\n",
      "        (original_module): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        )\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (k_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (v_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (o_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (up_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (down_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=10240, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bac858e7c423dcc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:38:52.217781Z",
     "start_time": "2025-08-21T11:38:52.085960Z"
    }
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb172501cce70556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:40:51.809824Z",
     "start_time": "2025-08-21T11:39:21.981456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['merged_model/processor_config.json']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdaf0e7263fba908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:41:06.476652Z",
     "start_time": "2025-08-21T11:40:53.533273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 40.69it/s]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Gemma3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "  args.output_dir,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  attn_implementation=\"eager\",\n",
    "  output_hidden_states=True,\n",
    "  output_attentions=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531bfd3c59f4745d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T04:48:52.781488Z",
     "start_time": "2025-08-12T04:48:52.773460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): PytorchGELUTanh()\n",
      "                (fc1): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=4304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (fc2): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4304, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): ModulesToSaveWrapper(\n",
      "        (original_module): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        )\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (k_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (o_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (up_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (down_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=10240, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfddaaed80b279d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T04:48:53.564126Z",
     "start_time": "2025-08-12T04:48:53.563136Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d957cd963eb25879",
   "metadata": {},
   "source": [
    "## try inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff066871c1d8c1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T04:48:54.961392Z",
     "start_time": "2025-08-12T04:48:54.335710Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "sample = {\n",
    "  \"product_name\": \"Hasbro Marvel Avengers-Serie Marvel Assemble Titan-Held, Iron Man, 30,5 cm Actionfigur\",\n",
    "  \"category\": \"Toys & Games | Toy Figures & Playsets | Action Figures\",\n",
    "  \"image\": Image.open(requests.get(\"https://m.media-amazon.com/images/I/81+7Up7IWyL._AC_SY300_SX300_.jpg\", stream=True).raw).convert(\"RGB\")\n",
    "}\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\",\"image\": sample[\"image\"]},\n",
    "        {\"type\": \"text\", \"text\": user_prompt.format(product=sample[\"product_name\"], category=sample[\"category\"])},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# Process the image and text\n",
    "image_inputs = process_vision_info(messages)\n",
    "# Tokenize the text and process the images\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# Move the inputs to the device\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # To get deterministic output (equivalent to temperature=0), we use greedy search by setting do_sample=False.\n",
    "    # The `temperature` parameter is ignored when do_sample is False.\n",
    "    # Note: The model.__call__ is a forward pass here, which doesn't perform generation.\n",
    "    # These generation parameters might be ignored. If you intend to generate text,\n",
    "    # you should use model.generate() instead.\n",
    "    output = model(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0868d8782abc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T04:48:56.054191Z",
     "start_time": "2025-08-12T04:48:56.052742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'hidden_states', 'attentions', 'image_hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f851f0b541794b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T04:48:56.946573Z",
     "start_time": "2025-08-12T04:48:56.945009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 371, 262208]) 35 34 1\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape,len(output.hidden_states),len(output.attentions),len(output.image_hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b763e61ba967",
   "metadata": {},
   "source": [
    "# collect distill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93a16fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:53:57.323564Z",
     "start_time": "2025-08-21T11:41:20.219316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and saving teacher signals to single CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 50/50 [10:25<00:00, 12.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 rows to dataset/distill_teacher_signals.csv\n"
     ]
    }
   ],
   "source": [
    "import os, io, base64, numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def to_tensor_list(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "def serialize_tensor_list(tensors) -> str:\n",
    "    if not tensors:\n",
    "        return \"\"\n",
    "    buf = io.BytesIO()\n",
    "    arrays = {f\"arr_{i}\": (t.squeeze(0).to(torch.float16).cpu().numpy() if isinstance(t, torch.Tensor) else np.array(t))\n",
    "              for i, t in enumerate(tensors)}\n",
    "    np.savez_compressed(buf, **arrays)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
    "\n",
    "# 確保會輸出所需中間特徵\n",
    "model.config.output_hidden_states = True\n",
    "model.config.output_attentions = True\n",
    "if hasattr(model.config, \"vision_config\"):\n",
    "    model.config.vision_config.output_hidden_states = True\n",
    "\n",
    "rows = []\n",
    "print(\"Generating and saving teacher signals to single CSV...\")\n",
    "for idx, sample in enumerate(tqdm(dataset, desc=\"Processing samples\")):\n",
    "    text_prompt = processor.apply_chat_template(\n",
    "        sample[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    ).strip()\n",
    "    image_inputs = process_vision_info(sample[\"messages\"])\n",
    "    inputs = processor(text=[text_prompt], images=[image_inputs], return_tensors=\"pt\", padding=False).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(**inputs)\n",
    "\n",
    "    hs_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"hidden_states\", None)))\n",
    "    attn_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"attentions\", None)))\n",
    "    img_hs_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"image_hidden_states\", None)))\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"teacher_hidden_states_b64\": hs_b64,\n",
    "        \"teacher_attentions_b64\": attn_b64,\n",
    "        \"teacher_image_hidden_states_b64\": img_hs_b64,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = \"dataset/distill_teacher_signals.csv\"  # 單檔 CSV（gzip壓縮）\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved {len(df)} rows to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65698751bd704c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:53:59.945217Z",
     "start_time": "2025-08-21T11:53:59.842279Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7787e3527c056",
   "metadata": {},
   "source": [
    "# Distill state"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3b1a2678c75f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:11:38.807504Z",
     "start_time": "2025-08-29T08:11:38.795506Z"
    }
   },
   "source": [
    "from torch.nn import MSELoss\n",
    "from trl import SFTTrainer\n",
    "from overrides import overrides\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BaseImageProcessor,\n",
    "    DataCollator,\n",
    "    FeatureExtractionMixin,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    ProcessorMixin,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_wandb_available,\n",
    ")\n",
    "\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n",
    "\n",
    "def get_cor_teacher(teacher_reps, student_reps, is_attn=False):\n",
    "    \"\"\"\n",
    "    Selects the corresponding teacher layers for the student layers.\n",
    "    This is used when the teacher model has more layers than the student model.\n",
    "    \"\"\"\n",
    "    #進來的是tuple，裡面是對應層數的Tensor\n",
    "    teacher_reps = [teacher_rep.detach() for teacher_rep in teacher_reps]\n",
    "    teacher_layer_num = len(teacher_reps)\n",
    "    student_layer_num = len(student_reps)\n",
    "    #print(teacher_reps[0].shape,student_reps[0].shape)#1,8,405,405\n",
    "    if is_attn:\n",
    "        # For attention layers\n",
    "        if teacher_layer_num % student_layer_num != 0:\n",
    "            raise ValueError(f\"Teacher attention layers ({teacher_layer_num}) not divisible by student's ({student_layer_num})\")\n",
    "        layers_per_block = teacher_layer_num // student_layer_num\n",
    "        # Select the last layer from each corresponding teacher block\n",
    "        new_teacher_reps = [teacher_reps[i * layers_per_block + layers_per_block - 1] for i in range(student_layer_num)]\n",
    "    else:\n",
    "        # For hidden states (including embeddings)\n",
    "        print(teacher_reps[0].shape,student_reps[0].shape)\n",
    "        if (teacher_layer_num - 1) % (student_layer_num - 1) != 0:\n",
    "            raise ValueError(f\"Teacher hidden layers ({teacher_layer_num - 1}) not divisible by student's ({student_layer_num - 1})\")\n",
    "        layers_per_block = (teacher_layer_num - 1) // (student_layer_num - 1)\n",
    "        # Select layers from the teacher at regular intervals, starting from the embeddings\n",
    "        new_teacher_reps = [teacher_reps[i * layers_per_block] for i in range(student_layer_num)]\n",
    "\n",
    "    return new_teacher_reps\n",
    "\n",
    "\n",
    "def get_kd_loss(student_reps, teacher_reps, loss_fn, is_attn=False, is_img=False):\n",
    "    \"\"\"\n",
    "    Computes the knowledge distillation loss between student and teacher representations.\n",
    "    \"\"\"\n",
    "    kd_loss = 0.0\n",
    "    if student_reps is None or teacher_reps is None:\n",
    "        return kd_loss\n",
    "\n",
    "    if is_attn:\n",
    "        for student_att, teacher_att in zip(student_reps, teacher_reps):\n",
    "            '''\n",
    "            if student_att.shape[1] != teacher_att.shape[1]:\n",
    "                min_len = min(student_att.shape[1], teacher_att.shape[1])\n",
    "                student_att = student_att[:, :min_len]\n",
    "                teacher_att = teacher_att[:, :min_len]\n",
    "            '''\n",
    "            student_att = torch.where(student_att <= -1e2, torch.zeros_like(student_att), student_att)\n",
    "            teacher_att = torch.where(teacher_att <= -1e2, torch.zeros_like(teacher_att), teacher_att)\n",
    "            kd_loss += loss_fn(student_att, teacher_att)\n",
    "            #print(\"att\")\n",
    "            #print(student_att.shape,teacher_att.shape)\n",
    "    elif is_img:\n",
    "        for student_rep, teacher_rep in zip(student_reps, teacher_reps):\n",
    "            teacher_rep = teacher_rep[0]\n",
    "            '''\n",
    "            if student_rep.shape[1] != teacher_rep.shape[1]:\n",
    "                min_len = min(student_rep.shape[1], teacher_rep.shape[1])\n",
    "                student_rep = student_rep[:, :min_len]\n",
    "                teacher_rep = teacher_rep[:, :min_len]\n",
    "            '''\n",
    "            #print(\"is_img\")\n",
    "            #print(student_rep.shape,teacher_rep.shape)\n",
    "            kd_loss += loss_fn(student_rep, teacher_rep)\n",
    "    else: # for hidden states\n",
    "        for student_rep, teacher_rep in zip(student_reps, teacher_reps):\n",
    "            '''\n",
    "            if student_rep.shape[1] != teacher_rep.shape[1]:\n",
    "                min_len = min(student_rep.shape[1], teacher_rep.shape[1])\n",
    "                student_rep = student_rep[:, :min_len]\n",
    "                teacher_rep = teacher_rep[:, :min_len]\n",
    "            '''\n",
    "            print(\"hidden states\")\n",
    "            print(student_rep.shape,teacher_rep.shape)\n",
    "            kd_loss += loss_fn(student_rep, teacher_rep)\n",
    "\n",
    "    return kd_loss\n",
    "\n",
    "class DistillSTFTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_hidden_states_loss(self, student_hidden_states, teacher_hidden_states,loss_fn):\n",
    "        if teacher_hidden_states is None:\n",
    "            return 0.0\n",
    "        # Align teacher and student hidden states\n",
    "        teacher_hidden_states = get_cor_teacher(teacher_hidden_states, student_hidden_states, is_attn=False)\n",
    "        return get_kd_loss(student_hidden_states, teacher_hidden_states, loss_fn, is_attn=False, is_img=False)\n",
    "\n",
    "    def compute_attentions_loss(self, student_attentions, teacher_attentions,loss_fn):\n",
    "        if teacher_attentions is None:\n",
    "            return 0.0\n",
    "        # Align teacher and student attentions\n",
    "        teacher_attentions = get_cor_teacher(teacher_attentions, student_attentions, is_attn=True)\n",
    "        return get_kd_loss(student_attentions, teacher_attentions, loss_fn, is_attn=True, is_img=False)\n",
    "\n",
    "    def compute_image_hidden_states_loss(self, student_image_hidden_states, teacher_image_hidden_states,loss_fn):\n",
    "        if teacher_image_hidden_states is None or student_image_hidden_states is None:\n",
    "            return 0.0\n",
    "        # Vision towers are identical, no layer alignment needed\n",
    "        return get_kd_loss(student_image_hidden_states, teacher_image_hidden_states, loss_fn, is_attn=False, is_img=True)\n",
    "\n",
    "    @overrides()\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
    "        # Pop the teacher's outputs from the inputs\n",
    "        teacher_hidden_states = inputs.pop(\"teacher_hidden_states\", None)\n",
    "        teacher_attentions = inputs.pop(\"teacher_attentions\", None)\n",
    "        teacher_image_hidden_states = inputs.pop(\"teacher_image_hidden_states\", None)\n",
    "\n",
    "\n",
    "        # Compute the original loss from SFTTrainer\n",
    "        loss, outputs = super().compute_loss(model, inputs, return_outputs=True,num_items_in_batch=num_items_in_batch)\n",
    "\n",
    "        # Get student's internal states\n",
    "        student_hidden_states = outputs.hidden_states\n",
    "        student_attentions = outputs.attentions\n",
    "        student_image_hidden_states = outputs.image_hidden_states\n",
    "\n",
    "        # Compute the distillation loss\n",
    "        mse_loss = MSELoss()\n",
    "        hidden_states_loss = self.compute_hidden_states_loss(student_hidden_states, teacher_hidden_states, mse_loss)\n",
    "        attentions_loss = self.compute_attentions_loss(student_attentions, teacher_attentions, mse_loss)\n",
    "        image_hidden_states_loss = self.compute_image_hidden_states_loss(student_image_hidden_states, teacher_image_hidden_states, mse_loss)\n",
    "        print(hidden_states_loss,attentions_loss,image_hidden_states_loss)\n",
    "        # Combine the losses (example: simple addition, could be weighted)\n",
    "        distill_loss = hidden_states_loss + attentions_loss + image_hidden_states_loss\n",
    "\n",
    "        # You can weigh the original loss and the distillation loss\n",
    "        # Example: loss = 0.4 * loss + 0.6 * distill_loss\n",
    "        loss += distill_loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "e1e7a028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:09:14.201233Z",
     "start_time": "2025-08-29T08:09:14.187878Z"
    }
   },
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# This collate_fn is designed to handle the output from our new distill_dataset\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    teacher_hidden_states_list = []\n",
    "    teacher_attentions_list = []\n",
    "    teacher_image_hidden_states_list = []\n",
    "\n",
    "    # 1. Extract data from each sample\n",
    "    for example in examples:\n",
    "        # Standard processing for text and images\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "        # Extract teacher outputs, ensuring they are not None\n",
    "        if \"teacher_hidden_states\" in example and example[\"teacher_hidden_states\"] is not None:\n",
    "            teacher_hidden_states_list.append(example[\"teacher_hidden_states\"])\n",
    "        if \"teacher_attentions\" in example and example[\"teacher_attentions\"] is not None:\n",
    "            teacher_attentions_list.append(example[\"teacher_attentions\"])\n",
    "        if \"teacher_image_hidden_states\" in example and example[\"teacher_image_hidden_states\"] is not None:\n",
    "            teacher_image_hidden_states_list.append(example[\"teacher_image_hidden_states\"])\n",
    "\n",
    "    # 2. Process and tokenize text and images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 3. Create labels, masking where necessary\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # 4. Pad and stack the teacher's outputs\n",
    "    if teacher_hidden_states_list:\n",
    "        padded_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_hidden_states_list):\n",
    "            # pad_sequence expects a list of tensors\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_hidden_states\"] = tuple(padded_hidden_states)\n",
    "\n",
    "    if teacher_attentions_list:\n",
    "        padded_attentions = []\n",
    "        for layer_tensors in zip(*teacher_attentions_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_attentions.append(padded_layer)\n",
    "        batch[\"teacher_attentions\"] = tuple(padded_attentions)\n",
    "\n",
    "    if teacher_image_hidden_states_list:\n",
    "        padded_image_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_image_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_image_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_image_hidden_states\"] = tuple(padded_image_hidden_states)\n",
    "\n",
    "    return batch\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "98e2c110cf65d720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:09:15.093096Z",
     "start_time": "2025-08-29T08:09:14.984955Z"
    }
   },
   "source": [
    "from trl import SFTConfig\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"model/gemma3-distill\",     # directory to save and repository id\n",
    "    num_train_epochs=1,                         # number of training epochs\n",
    "    per_device_train_batch_size=1,              # batch size per device during training\n",
    "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=5,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    ")\n",
    "args.remove_unused_columns = False # important for collator\n",
    "\n",
    "# This collate_fn is designed to handle the output from our new distill_dataset\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    teacher_hidden_states_list = []\n",
    "    teacher_attentions_list = []\n",
    "    teacher_image_hidden_states_list = []\n",
    "\n",
    "    # 1. Extract data from each sample\n",
    "    for example in examples:\n",
    "        # Standard processing for text and images\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "        # Extract teacher outputs, ensuring they are not None\n",
    "        if \"teacher_hidden_states\" in example and example[\"teacher_hidden_states\"] is not None:\n",
    "            teacher_hidden_states_list.append(example[\"teacher_hidden_states\"])\n",
    "        if \"teacher_attentions\" in example and example[\"teacher_attentions\"] is not None:\n",
    "            teacher_attentions_list.append(example[\"teacher_attentions\"])\n",
    "        if \"teacher_image_hidden_states\" in example and example[\"teacher_image_hidden_states\"] is not None:\n",
    "            teacher_image_hidden_states_list.append(example[\"teacher_image_hidden_states\"])\n",
    "\n",
    "    # 2. Process and tokenize text and images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 3. Create labels, masking where necessary\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # 4. Pad and stack the teacher's outputs\n",
    "    if teacher_hidden_states_list:\n",
    "        padded_hidden_states = []\n",
    "        # Transpose and pad each layer\n",
    "        for layer_tensors in zip(*teacher_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_hidden_states\"] = tuple(padded_hidden_states)\n",
    "\n",
    "    if teacher_attentions_list:\n",
    "        padded_attentions = []\n",
    "        for layer_tensors in zip(*teacher_attentions_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_attentions.append(padded_layer)\n",
    "        batch[\"teacher_attentions\"] = tuple(padded_attentions)\n",
    "\n",
    "    if teacher_image_hidden_states_list:\n",
    "        padded_image_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_image_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_image_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_image_hidden_states\"] = tuple(padded_image_hidden_states)\n",
    "\n",
    "    return batch"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "39db91883cba8018",
   "metadata": {},
   "source": [
    "## student model"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f25ed6788d75bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:10:10.260328Z",
     "start_time": "2025-08-29T08:09:17.883078Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, Gemma3ForConditionalGeneration,BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "# 1) 載入「原始 Gemma」(不要量化)\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU，安裝flash attn前必須先安裝ninja，詳細:https://blog.csdn.net/lckj2009/article/details/136054392\n",
    "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"cuda:0\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# 1) 載入「原始 Gemma」(不要量化)\n",
    "base_model_clean = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# 2) 建立半層數設定\n",
    "student_config = copy.deepcopy(base_model_clean.config)\n",
    "orig_layers = student_config.text_config.num_hidden_layers\n",
    "half_layers = orig_layers // 2\n",
    "student_config.text_config.num_hidden_layers = half_layers\n",
    "print(f\"Original decoder layers: {orig_layers} -> Student: {half_layers}\")\n",
    "\n",
    "# Explicitly enable outputting hidden states and attentions for the student model\n",
    "student_config.output_hidden_states = True\n",
    "student_config.output_attentions = True\n",
    "if hasattr(student_config, \"vision_config\"):\n",
    "    student_config.vision_config.output_hidden_states = True\n",
    "\n",
    "# 3) 建立學生模型（非量化）\n",
    "student_model = Gemma3ForConditionalGeneration(config=student_config).to(\n",
    "    base_model_clean.device, dtype=base_model_clean.dtype\n",
    ")\n",
    "\n",
    "# 4) 複製非層級權重\n",
    "with torch.no_grad():\n",
    "    # 視覺塔與投影器\n",
    "    student_model.model.vision_tower.load_state_dict(base_model_clean.model.vision_tower.state_dict())\n",
    "    student_model.model.multi_modal_projector.load_state_dict(base_model_clean.model.multi_modal_projector.state_dict())\n",
    "    # 文本嵌入與最終層正規化\n",
    "    student_model.model.language_model.embed_tokens.load_state_dict(\n",
    "        base_model_clean.model.language_model.embed_tokens.state_dict()\n",
    "    )\n",
    "    student_model.model.language_model.norm.load_state_dict(\n",
    "        base_model_clean.model.language_model.norm.state_dict()\n",
    "    )\n",
    "    # LM Head\n",
    "    student_model.lm_head.load_state_dict(base_model_clean.lm_head.state_dict())\n",
    "\n",
    "# 5) 定義兩層合一層的權重合併（逐元素平均）\n",
    "def average_state_dicts(sd_a: dict, sd_b: dict, alpha: float = 0.5) -> dict:\n",
    "    merged = {}\n",
    "    keys = sd_a.keys() & sd_b.keys()\n",
    "    for k in keys:\n",
    "        ta, tb = sd_a[k], sd_b[k]\n",
    "        if isinstance(ta, torch.Tensor) and isinstance(tb, torch.Tensor) and ta.shape == tb.shape:\n",
    "            # 只對浮點張量做平均，其餘沿用第一個\n",
    "            if torch.is_floating_point(ta) and torch.is_floating_point(tb):\n",
    "                merged[k] = (1 - alpha) * ta + alpha * tb\n",
    "            else:\n",
    "                merged[k] = ta\n",
    "        else:\n",
    "            merged[k] = ta\n",
    "    # 帶入 sd_a 獨有的鍵\n",
    "    for k in sd_a.keys() - keys:\n",
    "        merged[k] = sd_a[k]\n",
    "    return merged\n",
    "\n",
    "# 6) 以「每 2 層合成 1 層」方式拷貝到學生層\n",
    "with torch.no_grad():\n",
    "    for i in range(half_layers):\n",
    "        t_layer_a = base_model_clean.model.language_model.layers[2 * i]\n",
    "        t_layer_b = base_model_clean.model.language_model.layers[2 * i + 1]\n",
    "        s_layer = student_model.model.language_model.layers[i]\n",
    "\n",
    "        merged_sd = average_state_dicts(\n",
    "            t_layer_a.state_dict(),\n",
    "            t_layer_b.state_dict(),\n",
    "            alpha=1,  # 可調整權重比例\n",
    "        )\n",
    "        s_layer.load_state_dict(merged_sd, strict=False)\n",
    "with torch.no_grad():\n",
    "    student_model.lm_head.weight.data = student_model.model.language_model.embed_tokens.weight.data\n",
    "\n",
    "print(\"Student model built with 2-teacher-layers -> 1-student-layer merging.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original decoder layers: 34 -> Student: 17\n",
      "Student model built with 2-teacher-layers -> 1-student-layer merging.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "bc9d3a564b1b8c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:10:10.322830Z",
     "start_time": "2025-08-29T08:10:10.316845Z"
    }
   },
   "source": [
    "del base_model_clean\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "e8958ad6676e48df",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5222ee33ee67c",
   "metadata": {},
   "source": [
    "## 讀回蒸餾資料"
   ]
  },
  {
   "cell_type": "code",
   "id": "f124725cd2ddf69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:10:27.653946Z",
     "start_time": "2025-08-29T08:10:10.362944Z"
    }
   },
   "source": [
    "import pandas as pd, io, base64, numpy as np, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def deserialize_tensor_list(b64str):\n",
    "    if not isinstance(b64str, str) or b64str == \"\":\n",
    "        return None\n",
    "    data = base64.b64decode(b64str.encode(\"ascii\"))\n",
    "    buf = io.BytesIO(data)\n",
    "    with np.load(buf, allow_pickle=False) as npz:\n",
    "        keys = sorted(npz.files, key=lambda k: int(k.split(\"_\")[1]))\n",
    "        tensors = [torch.from_numpy(npz[k]).to(torch.float32) for k in keys]\n",
    "    return tensors\n",
    "\n",
    "class CSVTeacherSignalsDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, raw_dataset):\n",
    "        self.df = pd.read_csv(csv_path, compression=\"infer\")\n",
    "        self.raw_dataset = raw_dataset\n",
    "        assert self.df[\"id\"].max() < len(raw_dataset), \"CSV ids exceed raw dataset length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        raw = self.raw_dataset[int(row[\"id\"])]\n",
    "        return {\n",
    "            \"messages\": raw[\"messages\"],\n",
    "            \"teacher_hidden_states\": deserialize_tensor_list(row.get(\"teacher_hidden_states_b64\", \"\")),\n",
    "            \"teacher_attentions\": deserialize_tensor_list(row.get(\"teacher_attentions_b64\", \"\")),\n",
    "            \"teacher_image_hidden_states\": deserialize_tensor_list(row.get(\"teacher_image_hidden_states_b64\", \"\")),\n",
    "        }\n",
    "\n",
    "distill_dataset = CSVTeacherSignalsDataset(\"dataset/distill_teacher_signals.csv\", dataset)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f0065fd1b9470f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:10:31.720988Z",
     "start_time": "2025-08-29T08:10:27.687632Z"
    }
   },
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "7b442fb4f03fb09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:11:54.468731Z",
     "start_time": "2025-08-29T08:11:53.618387Z"
    }
   },
   "source": [
    "trainer = DistillSTFTrainer(\n",
    "    model=student_model,\n",
    "    args=args,\n",
    "    train_dataset=distill_dataset,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "8a437d39f2468694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:12:11.598008Z",
     "start_time": "2025-08-29T08:11:56.663283Z"
    }
   },
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model(safe_serialization=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "tensor(inf, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 429, 2560]) torch.Size([1, 429, 2560])\n",
      "tensor(inf, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0108, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 445, 2560]) torch.Size([1, 445, 2560])\n",
      "tensor(inf, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "tensor(inf, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/9 : < :, Epoch 0.12/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "hidden states\n",
      "torch.Size([1, 436, 2560]) torch.Size([1, 436, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 662.12 MiB is free. Process 992382 has 22.75 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 545.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start training, the model will be automatically saved to the Hub and the output directory\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Save the final model again to the Hugging Face Hub\u001B[39;00m\n\u001B[1;32m      5\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model(safe_serialization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2238\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2236\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2238\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2239\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2243\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2582\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2575\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2576\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2577\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2578\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2579\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2580\u001B[0m )\n\u001B[1;32m   2581\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2582\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2585\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2586\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2587\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2588\u001B[0m ):\n\u001B[1;32m   2589\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2590\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:904\u001B[0m, in \u001B[0;36mSFTTrainer.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaybe_activation_offload_context:\n\u001B[0;32m--> 904\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3845\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3842\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED:\n\u001B[1;32m   3843\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_wrt_gas\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 3845\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3847\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2734\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2733\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2734\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1124\u001B[0m, in \u001B[0;36m_checkpoint_hook.__init__.<locals>.unpack_hook\u001B[0;34m(holder)\u001B[0m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _recomputation_hook(\n\u001B[1;32m   1122\u001B[0m         weakref\u001B[38;5;241m.\u001B[39mref(frame), gid\n\u001B[1;32m   1123\u001B[0m     ), torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m-> 1124\u001B[0m         \u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecompute_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _StopRecomputationError:\n\u001B[1;32m   1126\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1518\u001B[0m, in \u001B[0;36m_checkpoint_without_reentrant_generator.<locals>.recompute_fn\u001B[0;34m(*inputs)\u001B[0m\n\u001B[1;32m   1514\u001B[0m device_autocast_ctx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast(\n\u001B[1;32m   1515\u001B[0m     device_type\u001B[38;5;241m=\u001B[39mdevice_type, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_autocast_kwargs\n\u001B[1;32m   1516\u001B[0m ) \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mis_autocast_available(device_type) \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext()\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m device_autocast_ctx, torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcpu_autocast_kwargs), recompute_context:  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1518\u001B[0m     \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:450\u001B[0m, in \u001B[0;36mSiglipEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    447\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    449\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm1(hidden_states)\n\u001B[0;32m--> 450\u001B[0m hidden_states, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    457\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:389\u001B[0m, in \u001B[0;36mSiglipAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    387\u001B[0m     attention_interface \u001B[38;5;241m=\u001B[39m ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation]\n\u001B[0;32m--> 389\u001B[0m attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    400\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(batch_size, seq_length, embed_dim)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    401\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj(attn_output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:335\u001B[0m, in \u001B[0;36meager_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    333\u001B[0m     attn_weights \u001B[38;5;241m=\u001B[39m attn_weights \u001B[38;5;241m+\u001B[39m attention_mask\n\u001B[0;32m--> 335\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(query\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    336\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(attn_weights, p\u001B[38;5;241m=\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39mmodule\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    338\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(attn_weights, value)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2142\u001B[0m, in \u001B[0;36msoftmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   2140\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(dim)\n\u001B[1;32m   2141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2142\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 662.12 MiB is free. Process 992382 has 22.75 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 545.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0aae556ebfc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a229a2",
   "metadata": {},
   "source": [
    "# Test Lora"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f0df0e7ffc78079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T08:09:39.003176Z",
     "start_time": "2025-08-26T08:09:37.252282Z"
    }
   },
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# --- 請在此處設定您的模型路徑 ---\n",
    "# 基礎模型路徑 (與訓練時使用的模型一致)\n",
    "BASE_MODEL_PATH = \"google/gemma-3-4b-pt\"\n",
    "# Processor ID (通常是 instruction-tuned 版本，包含聊天範本)\n",
    "PROCESSOR_ID = \"google/gemma-3-4b-it\"\n",
    "# LoRA 適配器權重存放的路徑 (使用此筆記本訓練後儲存的路徑)\n",
    "LORA_ADAPTER_PATH = \"model/gemma-test\"\n",
    "# -----------------------------------------\n",
    "\n",
    "# 檢查路徑是否存在\n",
    "if not os.path.exists(LORA_ADAPTER_PATH):\n",
    "    raise FileNotFoundError(f\"LoRA 適配器路徑不存在: {LORA_ADAPTER_PATH}\")\n",
    "\n",
    "# 設定裝置\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用的裝置: {device}\")\n",
    "# 建議使用 bfloat16 以節省記憶體\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32\n",
    "\n",
    "# --- 1. 準備輸入資料 (使用資料集的第一筆) ---\n",
    "print(\"\\n--- 準備輸入資料 ---\")\n",
    "# 確保 'dataset' 變數已經從前面的儲存格載入\n",
    "try:\n",
    "    first_sample = dataset[0]\n",
    "except NameError:\n",
    "    raise NameError(\"'dataset' not found. Please make sure to run the cells that load the dataset first.\")\n",
    "\n",
    "# 載入此測試專用的 processor\n",
    "processor_for_test = AutoProcessor.from_pretrained(PROCESSOR_ID)\n",
    "\n",
    "# 使用 `apply_chat_template` 產生包含特殊 token (例如 <image>) 的正確 prompt\n",
    "prompt_with_token = processor_for_test.apply_chat_template(\n",
    "    first_sample['messages'],\n",
    "    add_generation_prompt=False, # 與訓練時的 collate_fn 保持一致\n",
    "    tokenize=False\n",
    ").strip()\n",
    "\n",
    "# 使用 `process_vision_info` (已在筆記本前面定義) 來提取圖片\n",
    "images = process_vision_info(first_sample['messages'])\n",
    "if not images:\n",
    "    raise ValueError(\"在資料集的第一筆資料中找不到圖片。\")\n",
    "\n",
    "print(\"使用資料集第一筆資料進行測試...\")\n",
    "print(f\"Image: {images[0]}\")\n",
    "\n",
    "# 將格式化後的 prompt 和圖片傳給 processor\n",
    "inputs = processor_for_test(text=prompt_with_token, images=images, return_tensors=\"pt\").to(device, dtype=torch_dtype)\n",
    "\n",
    "print(\"輸入資料準備完成。\")\n",
    "\n",
    "# --- 2. 獲取基礎模型 (Base Model) 的輸出 ---\n",
    "print(\"\\n--- 正在載入基礎模型並取得 Hidden State 和 Attention ---\")\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch_dtype, device_map=device)\n",
    "with torch.no_grad():\n",
    "    outputs_base = base_model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "hidden_states_base = outputs_base.hidden_states\n",
    "# 只取最後一層 attention\n",
    "attentions_base = outputs_base.attentions[-1].cpu() if outputs_base.attentions else None\n",
    "print(\"基礎模型的 Hidden State 和 Attention 已取得。\")\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- 3. 獲取應用 LoRA 後 (未合併) 的輸出 ---\n",
    "print(\"\\n--- 正在載入帶有 LoRA 適配器的模型並取得 Hidden State 和 Attention ---\")\n",
    "lora_base_model = AutoModelForImageTextToText.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch_dtype, device_map=device)\n",
    "lora_model = PeftModel.from_pretrained(lora_base_model, LORA_ADAPTER_PATH)\n",
    "lora_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_lora = lora_model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "hidden_states_lora = outputs_lora.hidden_states\n",
    "# 只取最後一層 attention\n",
    "attentions_lora = outputs_lora.attentions.cpu() if outputs_lora.attentions else None\n",
    "print(\"LoRA 適配器模型的 Hidden State 和 Attention 已取得。\")\n",
    "del lora_base_model, lora_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- 4. 獲取合併 LoRA 後的模型 (Merged Model) 的輸出 ---\n",
    "print(\"\\n--- 正在載入並合併 LoRA 適配器，然後取得 Hidden State 和 Attention ---\")\n",
    "merged_base_model = AutoModelForImageTextToText.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch_dtype, device_map=device)\n",
    "peft_model_for_merge = PeftModel.from_pretrained(merged_base_model, LORA_ADAPTER_PATH)\n",
    "merged_model = peft_model_for_merge.merge_and_unload()\n",
    "merged_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_merged = merged_model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "hidden_states_merged = outputs_merged.hidden_states\n",
    "# 只取最後一層 attention\n",
    "attentions_merged = outputs_merged.attentions[-1].cpu() if outputs_merged.attentions else None\n",
    "print(\"合併後模型的 Hidden State 和 Attention 已取得。\")\n",
    "del merged_base_model, merged_model, peft_model_for_merge\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- 5. 定義逐層比較函式 ---\n",
    "def compare_layers(tensors_a, tensors_b, model_a_name, model_b_name, tensor_type, expected_same, atol=1e-5, rtol=1e-4):\n",
    "    \"\"\"\n",
    "    逐層比較兩個模型的張量輸出 (hidden states 或 attentions)。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 比較 {model_a_name} vs. {model_b_name} 的 {tensor_type} (預期: {'相同' if expected_same else '不同'}) ---\")\n",
    "    if not tensors_a or not tensors_b:\n",
    "        print(\"  - 驗證失敗：其中一個模型的輸出為空。\")\n",
    "        return False\n",
    "\n",
    "    if len(tensors_a) != len(tensors_b):\n",
    "        print(f\"  - 驗證失敗：層數不匹配。 {model_a_name} ({len(tensors_a)} 層) vs {model_b_name} ({len(tensors_b)} 層)。\")\n",
    "        return False\n",
    "\n",
    "    overall_result_matches_expectation = True\n",
    "\n",
    "    for i in range(len(tensors_a)):\n",
    "        layer_a = tensors_a[i]\n",
    "        layer_b = tensors_b[i]\n",
    "\n",
    "        # 處理模型輸出中可能存在的 None 值\n",
    "        if layer_a is None or layer_b is None:\n",
    "            print(f\"  - [警告] 第 {i} 層被跳過，因為至少一個模型的輸出為 None。 (A: {type(layer_a)}, B: {type(layer_b)})\")\n",
    "            # 如果預期相同，但一個是 None 另一個不是，則不符合預期\n",
    "            if expected_same and (type(layer_a) is not type(layer_b)):\n",
    "                overall_result_matches_expectation = False\n",
    "            continue\n",
    "\n",
    "        layer_a = layer_a.cpu()\n",
    "        layer_b = layer_b.cpu()\n",
    "\n",
    "        # 處理因 padding 不同可能造成的形狀差異，截斷至最小長度\n",
    "        if layer_a.shape != layer_b.shape:\n",
    "            print(f\"  - [警告] 第 {i} 層張量形狀不同，將截斷以進行比較: {layer_a.shape} vs {layer_b.shape}\")\n",
    "            try:\n",
    "                # 通用截斷邏輯，適用於 (batch, ..., seq_len, dim)\n",
    "                min_seq_len = min(layer_a.shape[-2], layer_b.shape[-2])\n",
    "                if layer_a.dim() == 3: # Hidden States\n",
    "                     layer_a = layer_a[:, :min_seq_len, :]\n",
    "                     layer_b = layer_b[:, :min_seq_len, :]\n",
    "                elif layer_a.dim() == 4: # Attentions\n",
    "                     layer_a = layer_a[..., :min_seq_len, :min_seq_len]\n",
    "                     layer_b = layer_b[..., :min_seq_len, :min_seq_len]\n",
    "            except IndexError:\n",
    "                 print(f\"  - [錯誤] 無法處理第 {i} 層的形狀差異，跳過比較。\")\n",
    "                 continue\n",
    "\n",
    "        are_same_this_layer = torch.allclose(layer_a, layer_b, atol=atol, rtol=rtol)\n",
    "\n",
    "        # 檢查該層的結果是否符合整體的預期\n",
    "        if are_same_this_layer != expected_same:\n",
    "            overall_result_matches_expectation = False\n",
    "            # 只顯示不符合預期的結果，讓輸出更簡潔\n",
    "            print(f\"  - [不符預期] 第 {i} 層比較結果: {'相同' if are_same_this_layer else '不同'}\")\n",
    "\n",
    "    if overall_result_matches_expectation:\n",
    "        print(f\"  - 驗證成功：整體驗證結果符合預期。\")\n",
    "    else:\n",
    "        print(f\"  - 驗證失敗：整體驗證結果不符合預期。\")\n",
    "\n",
    "    return overall_result_matches_expectation\n",
    "\n",
    "# --- 6. 逐層比較 Hidden States ---\n",
    "print(\"\\n\\n--- Hidden State 逐層結果比較 ---\")\n",
    "compare_layers(hidden_states_base, hidden_states_lora, \"基礎模型\", \"LoRA模型\", \"Hidden States\", expected_same=False)\n",
    "compare_layers(hidden_states_lora, hidden_states_merged, \"LoRA模型\", \"合併後模型\", \"Hidden States\", expected_same=True)\n",
    "compare_layers(hidden_states_base, hidden_states_merged, \"基礎模型\", \"合併後模型\", \"Hidden States\", expected_same=False)\n",
    "\n",
    "# --- 7. 比較 Attentions (僅最後一層) ---\n",
    "print(\"\\n\\n--- Attention 結果比較 (僅比較最後一層) ---\")\n",
    "\n",
    "if attentions_base is not None and attentions_lora is not None and attentions_merged is not None:\n",
    "    # 比較 Base vs LoRA (預期不同)\n",
    "    are_base_lora_attentions_same = torch.allclose(attentions_base, attentions_lora)\n",
    "    print(f\"基礎模型 vs. LoRA 模型 (未合併) 的 Attentions 是否完全相同? -> {are_base_lora_attentions_same}\")\n",
    "    if not are_base_lora_attentions_same:\n",
    "        print(\"  - 驗證成功：LoRA 適配器改變了模型的 Attention。\")\n",
    "    else:\n",
    "        print(\"  - 驗證失敗：LoRA 適配器的 Attention 沒有生效。\")\n",
    "\n",
    "    # 比較 LoRA vs Merged (預期相同)\n",
    "    are_lora_merged_attentions_same = torch.allclose(attentions_lora, attentions_merged, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"LoRA 模型 (未合併) vs. 合併後模型的 Attentions 是否幾乎相同? -> {are_lora_merged_attentions_same}\")\n",
    "    if are_lora_merged_attentions_same:\n",
    "        print(\"  - 驗證成功：合併後的模型與動態應用 LoRA 的模型 Attention 一致。\")\n",
    "    else:\n",
    "        diff = torch.abs(attentions_lora - attentions_merged).max()\n",
    "        print(f\"  - 驗證失敗：合併過程可能存在 Attention 數值精度差異。最大差異值: {diff}\")\n",
    "\n",
    "    # 比較 Base vs Merged (預期不同)\n",
    "    are_base_merged_attentions_same = torch.allclose(attentions_base, attentions_merged)\n",
    "    print(f\"基礎模型 vs. 合併後模型的 Attentions 是否完全相同? -> {are_base_merged_attentions_same}\")\n",
    "    if not are_base_merged_attentions_same:\n",
    "        print(\"  - 驗證成功：合併後的模型確實包含了 LoRA 的權重更新（反映在 Attention 上）。\")\n",
    "    else:\n",
    "        print(\"  - 驗證失敗：合併後模型的 Attention 與基礎模型無異。\")\n",
    "else:\n",
    "    print(\"  - 驗證失敗：無法取得所有模型的 Attention 輸出。\")\n",
    "\n",
    "print(\"\\n\\n驗證完成。\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的裝置: cuda\n",
      "\n",
      "--- 準備輸入資料 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "'dataset' not found. Please make sure to run the cells that load the dataset first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 30\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     first_sample \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dataset' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 32\u001B[0m\n\u001B[1;32m     30\u001B[0m     first_sample \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not found. Please make sure to run the cells that load the dataset first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# 載入此測試專用的 processor\u001B[39;00m\n\u001B[1;32m     35\u001B[0m processor_for_test \u001B[38;5;241m=\u001B[39m AutoProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(PROCESSOR_ID)\n",
      "\u001B[0;31mNameError\u001B[0m: 'dataset' not found. Please make sure to run the cells that load the dataset first."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a128417379891e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
