{
 "cells": [
  {
   "cell_type": "code",
   "id": "a718ff9cb8d75e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:45:16.927971Z",
     "start_time": "2025-08-24T09:45:16.260200Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "import base64"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "exp_name = \"gemma3-4b-sft-textonly-\"\n",
    "'''\n",
    "本次實驗名稱\n",
    "textonly:對應dataset/0513_SFTDataset/text/qa_pairs_sft.json，只包含文字問題\n",
    "hitdata:對應dataset/0513_SFTDataset/hitdata/sft_training_data.json，為圖文對資料\n",
    "'''"
   ],
   "id": "d8a4a094df0b6066"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_locate = \"/tmp/pycharm_project_979/\" #遠端環境，根據部署地做更改\n",
    "jsonFile_path = file_locate+\"dataset/0513_SFTDataset/text/qa_pairs_sft.json\"\n",
    "\n",
    "with open(jsonFile_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)"
   ],
   "id": "3a833d1c8407e235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b65f1a244ff8371e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 微調階段",
   "id": "5842f7de7e31d058"
  },
  {
   "cell_type": "code",
   "id": "c4fdb698e81c482b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:45:26.451408Z",
     "start_time": "2025-08-24T09:45:23.192515Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cad42709ed66437d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:45:41.153214Z",
     "start_time": "2025-08-24T09:45:26.628570Z"
    }
   },
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import PartialState\n",
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-4b-pt\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "device_string = PartialState().process_index\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU，安裝flash attn前必須先安裝ninja，詳細:https://blog.csdn.net/lckj2009/article/details/136054392\n",
    "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
    "    device_map={'':device_string}\n",
    ")\n",
    "\n",
    " #BitsAndBytesConfig int-4 config\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e6f720375af74532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:35:31.590557Z",
     "start_time": "2025-08-22T09:35:31.585185Z"
    }
   },
   "source": "print(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): PytorchGELUTanh()\n",
      "                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
      "                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "da4759e01ae2c21f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:45:44.995544Z",
     "start_time": "2025-08-24T09:45:44.892251Z"
    }
   },
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "f41b76590777ee4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:46:08.322242Z",
     "start_time": "2025-08-24T09:46:08.275661Z"
    }
   },
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"model/gemma-test\",     # directory to save and repository id\n",
    "    num_train_epochs=1,                         # number of training epochs\n",
    "    per_device_train_batch_size=4,              # batch size per device during training\n",
    "    gradient_accumulation_steps=1,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=5,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    ")\n",
    "args.remove_unused_columns = False # important for collator\n",
    "\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels.cpu()\n",
    "    return batch"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "87a9d0ec7f3ad5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:46:14.902797Z",
     "start_time": "2025-08-24T09:46:12.672577Z"
    }
   },
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    "\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-24 09:46:13,748] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-24 09:46:14,868] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "7dd7bb0ff560772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T09:46:23.511445Z",
     "start_time": "2025-08-24T09:46:19.020308Z"
    }
   },
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start training, the model will be automatically saved to the Hub and the output directory\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Save the final model again to the Hugging Face Hub\u001B[39;00m\n\u001B[1;32m      5\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2238\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2236\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2238\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2239\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2243\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2582\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2575\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2576\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2577\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2578\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2579\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2580\u001B[0m )\n\u001B[1;32m   2581\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2582\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2585\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2586\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2587\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2588\u001B[0m ):\n\u001B[1;32m   2589\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2590\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:904\u001B[0m, in \u001B[0;36mSFTTrainer.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaybe_activation_offload_context:\n\u001B[0;32m--> 904\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3796\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3793\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3795\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3796\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3798\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3801\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3802\u001B[0m ):\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:865\u001B[0m, in \u001B[0;36mSFTTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    862\u001B[0m     \u001B[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001B[39;00m\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001B[39;00m\n\u001B[1;32m    864\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs:\n\u001B[0;32m--> 865\u001B[0m         num_tokens_in_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather_for_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mposition_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs:\n\u001B[1;32m    867\u001B[0m         local_num_tokens \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mposition_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m), device\u001B[38;5;241m=\u001B[39minputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mposition_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2994\u001B[0m, in \u001B[0;36mAccelerator.gather_for_metrics\u001B[0;34m(self, input_data, use_gather_object)\u001B[0m\n\u001B[1;32m   2992\u001B[0m     data \u001B[38;5;241m=\u001B[39m gather_object(input_data)\n\u001B[1;32m   2993\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2994\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2996\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2997\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_state\u001B[38;5;241m.\u001B[39mend_of_dataloader:\n\u001B[1;32m   2998\u001B[0m         \u001B[38;5;66;03m# at the end of a dataloader, `gather_for_metrics` regresses to\u001B[39;00m\n\u001B[1;32m   2999\u001B[0m         \u001B[38;5;66;03m# `gather` unless the dataset has a remainder so log.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2950\u001B[0m, in \u001B[0;36mAccelerator.gather\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgather\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor):\n\u001B[1;32m   2921\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2922\u001B[0m \u001B[38;5;124;03m    Gather the values in *tensor* across all processes and concatenate them on the first dimension. Useful to\u001B[39;00m\n\u001B[1;32m   2923\u001B[0m \u001B[38;5;124;03m    regroup the predictions from all processes when doing evaluation.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2948\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m   2949\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2950\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:371\u001B[0m, in \u001B[0;36mverify_operation.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m PartialState()\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mNO \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m PartialState()\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[0;32m--> 371\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    372\u001B[0m     operation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunction\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunction\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensor\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:432\u001B[0m, in \u001B[0;36mgather\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tpu_gather(tensor)\n\u001B[1;32m    431\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m PartialState()\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;129;01min\u001B[39;00m TORCH_DISTRIBUTED_OPERATION_TYPES:\n\u001B[0;32m--> 432\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_gpu_gather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tensor\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:351\u001B[0m, in \u001B[0;36m_gpu_gather\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    348\u001B[0m         torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mall_gather(output_tensors, tensor)\n\u001B[1;32m    349\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_tensors, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrecursively_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_gpu_gather_one\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_on_other_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:126\u001B[0m, in \u001B[0;36mrecursively_apply\u001B[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(data)(\n\u001B[1;32m    118\u001B[0m         {\n\u001B[1;32m    119\u001B[0m             k: recursively_apply(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    123\u001B[0m         }\n\u001B[1;32m    124\u001B[0m     )\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m test_type(data):\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m error_on_other_type:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported types (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) passed to `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`. Only nested list/tuple/dicts of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjects that are valid for `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_type\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` should be passed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:348\u001B[0m, in \u001B[0;36m_gpu_gather.<locals>._gpu_gather_one\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;66;03m# a backend of `None` is always CPU\u001B[39;00m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;66;03m# also gloo does not support `all_gather_into_tensor`,\u001B[39;00m\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;66;03m# which will result in a larger memory overhead for the op\u001B[39;00m\n\u001B[1;32m    347\u001B[0m     output_tensors \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mempty_like(tensor) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(state\u001B[38;5;241m.\u001B[39mnum_processes)]\n\u001B[0;32m--> 348\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_gather\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_tensors, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\u001B[0m, in \u001B[0;36m_exception_logger.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _T:\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m     83\u001B[0m         msg_dict \u001B[38;5;241m=\u001B[39m _get_msg_dict(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3727\u001B[0m, in \u001B[0;36mall_gather\u001B[0;34m(tensor_list, tensor, group, async_op)\u001B[0m\n\u001B[1;32m   3722\u001B[0m tensor_list \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3723\u001B[0m     t \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mview_as_real(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tensor_list\n\u001B[1;32m   3724\u001B[0m ]\n\u001B[1;32m   3725\u001B[0m tensor \u001B[38;5;241m=\u001B[39m tensor \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mview_as_real(tensor)\n\u001B[0;32m-> 3727\u001B[0m group \u001B[38;5;241m=\u001B[39m group \u001B[38;5;129;01mor\u001B[39;00m \u001B[43m_get_default_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3728\u001B[0m work \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39mallgather([tensor_list], [tensor])\n\u001B[1;32m   3730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m async_op:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:1298\u001B[0m, in \u001B[0;36m_get_default_group\u001B[0;34m()\u001B[0m\n\u001B[1;32m   1296\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001B[39;00m\n\u001B[1;32m   1297\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_initialized():\n\u001B[0;32m-> 1298\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1299\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDefault process group has not been initialized, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1300\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease make sure to call init_process_group.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1301\u001B[0m     )\n\u001B[1;32m   1302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m   1303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m not_none(GroupMember\u001B[38;5;241m.\u001B[39mWORLD)\n",
      "\u001B[0;31mValueError\u001B[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0fe6e9e06041f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:00:22.548425Z",
     "start_time": "2025-08-11T12:00:22.540518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): PytorchGELUTanh()\n",
      "                (fc1): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=4304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (fc2): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4304, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): ModulesToSaveWrapper(\n",
      "        (original_module): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "        )\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (k_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (v_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (o_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (up_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=2560, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=10240, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (down_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=10240, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=2560, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bac858e7c423dcc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:38:52.217781Z",
     "start_time": "2025-08-21T11:38:52.085960Z"
    }
   },
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 合併Lora",
   "id": "a8cc4d33e0616e71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:40:51.809824Z",
     "start_time": "2025-08-21T11:39:21.981456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")"
   ],
   "id": "cb172501cce70556",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['merged_model/processor_config.json']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##　載入已微調模型",
   "id": "a3522b7cbd74339e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:41:06.476652Z",
     "start_time": "2025-08-21T11:40:53.533273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Gemma3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "  args.output_dir,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  attn_implementation=\"eager\",\n",
    "  output_hidden_states=True,\n",
    "  output_attentions=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)"
   ],
   "id": "fdaf0e7263fba908",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 40.69it/s]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# collect distill data",
   "id": "9c5b763e61ba967"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:53:57.323564Z",
     "start_time": "2025-08-21T11:41:20.219316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, io, base64, numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def to_tensor_list(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "def serialize_tensor_list(tensors) -> str:\n",
    "    if not tensors:\n",
    "        return \"\"\n",
    "    buf = io.BytesIO()\n",
    "    arrays = {f\"arr_{i}\": (t.squeeze(0).to(torch.float16).cpu().numpy() if isinstance(t, torch.Tensor) else np.array(t))\n",
    "              for i, t in enumerate(tensors)}\n",
    "    np.savez_compressed(buf, **arrays)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
    "\n",
    "# 確保會輸出所需中間特徵\n",
    "model.config.output_hidden_states = True\n",
    "model.config.output_attentions = True\n",
    "if hasattr(model.config, \"vision_config\"):\n",
    "    model.config.vision_config.output_hidden_states = True\n",
    "\n",
    "rows = []\n",
    "print(\"Generating and saving teacher signals to single CSV...\")\n",
    "for idx, sample in enumerate(tqdm(dataset, desc=\"Processing samples\")):\n",
    "    text_prompt = processor.apply_chat_template(\n",
    "        sample[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    ).strip()\n",
    "    image_inputs = process_vision_info(sample[\"messages\"])\n",
    "    inputs = processor(text=[text_prompt], images=[image_inputs], return_tensors=\"pt\", padding=False).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(**inputs)\n",
    "\n",
    "    hs_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"hidden_states\", None)))\n",
    "    attn_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"attentions\", None)))\n",
    "    img_hs_b64 = serialize_tensor_list(to_tensor_list(getattr(out, \"image_hidden_states\", None)))\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"teacher_hidden_states_b64\": hs_b64,\n",
    "        \"teacher_attentions_b64\": attn_b64,\n",
    "        \"teacher_image_hidden_states_b64\": img_hs_b64,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = \"dataset/distill_teacher_signals.csv\"  # 單檔 CSV（gzip壓縮）\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved {len(df)} rows to {csv_path}\")"
   ],
   "id": "93a16fa9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and saving teacher signals to single CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 50/50 [10:25<00:00, 12.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 rows to dataset/distill_teacher_signals.csv\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T11:53:59.945217Z",
     "start_time": "2025-08-21T11:53:59.842279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "65698751bd704c7b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Distill state",
   "id": "8da7787e3527c056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:41:50.127676Z",
     "start_time": "2025-08-22T07:41:44.554761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn import MSELoss\n",
    "from trl import SFTTrainer\n",
    "from overrides import overrides\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BaseImageProcessor,\n",
    "    DataCollator,\n",
    "    FeatureExtractionMixin,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    ProcessorMixin,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_wandb_available,\n",
    ")\n",
    "\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n",
    "\n",
    "def get_cor_teacher(teacher_reps, student_reps, is_attn=False):\n",
    "    \"\"\"\n",
    "    Selects the corresponding teacher layers for the student layers.\n",
    "    This is used when the teacher model has more layers than the student model.\n",
    "    \"\"\"\n",
    "    #進來的是tuple，裡面是對應層數的Tensor\n",
    "    teacher_reps = [teacher_rep.detach() for teacher_rep in teacher_reps]\n",
    "    teacher_layer_num = len(teacher_reps)\n",
    "    student_layer_num = len(student_reps)\n",
    "    #print(teacher_reps[0].shape,student_reps[0].shape)#1,8,405,405\n",
    "    if is_attn:\n",
    "        # For attention layers\n",
    "        if teacher_layer_num % student_layer_num != 0:\n",
    "            raise ValueError(f\"Teacher attention layers ({teacher_layer_num}) not divisible by student's ({student_layer_num})\")\n",
    "        layers_per_block = teacher_layer_num // student_layer_num\n",
    "        # Select the last layer from each corresponding teacher block\n",
    "        new_teacher_reps = [teacher_reps[i * layers_per_block + layers_per_block - 1] for i in range(student_layer_num)]\n",
    "    else:\n",
    "        # For hidden states (including embeddings)\n",
    "        print(teacher_reps[0].shape,student_reps[0].shape)\n",
    "        if (teacher_layer_num - 1) % (student_layer_num - 1) != 0:\n",
    "            raise ValueError(f\"Teacher hidden layers ({teacher_layer_num - 1}) not divisible by student's ({student_layer_num - 1})\")\n",
    "        layers_per_block = (teacher_layer_num - 1) // (student_layer_num - 1)\n",
    "        # Select layers from the teacher at regular intervals, starting from the embeddings\n",
    "        new_teacher_reps = [teacher_reps[i * layers_per_block] for i in range(student_layer_num)]\n",
    "\n",
    "    return new_teacher_reps\n",
    "\n",
    "\n",
    "def get_kd_loss(student_reps, teacher_reps, loss_fn, is_attn=False, is_img=False):\n",
    "    \"\"\"\n",
    "    Computes the knowledge distillation loss between student and teacher representations.\n",
    "    \"\"\"\n",
    "    kd_loss = 0.0\n",
    "    if student_reps is None or teacher_reps is None:\n",
    "        return kd_loss\n",
    "\n",
    "    if is_attn:\n",
    "        for student_att, teacher_att in zip(student_reps, teacher_reps):\n",
    "            '''\n",
    "            if student_att.shape[1] != teacher_att.shape[1]:\n",
    "                min_len = min(student_att.shape[1], teacher_att.shape[1])\n",
    "                student_att = student_att[:, :min_len]\n",
    "                teacher_att = teacher_att[:, :min_len]\n",
    "            '''\n",
    "            student_att = torch.where(student_att <= -1e2, torch.zeros_like(student_att), student_att)\n",
    "            teacher_att = torch.where(teacher_att <= -1e2, torch.zeros_like(teacher_att), teacher_att)\n",
    "            kd_loss += loss_fn(student_att, teacher_att)\n",
    "            #print(\"att\")\n",
    "            #print(student_att.shape,teacher_att.shape)\n",
    "    elif is_img:\n",
    "        for student_rep, teacher_rep in zip(student_reps, teacher_reps):\n",
    "            teacher_rep = teacher_rep[0]\n",
    "            '''\n",
    "            if student_rep.shape[1] != teacher_rep.shape[1]:\n",
    "                min_len = min(student_rep.shape[1], teacher_rep.shape[1])\n",
    "                student_rep = student_rep[:, :min_len]\n",
    "                teacher_rep = teacher_rep[:, :min_len]\n",
    "            '''\n",
    "            #print(\"is_img\")\n",
    "            #print(student_rep.shape,teacher_rep.shape)\n",
    "            kd_loss += loss_fn(student_rep, teacher_rep)\n",
    "    else: # for hidden states\n",
    "        for student_rep, teacher_rep in zip(student_reps, teacher_reps):\n",
    "            '''\n",
    "            if student_rep.shape[1] != teacher_rep.shape[1]:\n",
    "                min_len = min(student_rep.shape[1], teacher_rep.shape[1])\n",
    "                student_rep = student_rep[:, :min_len]\n",
    "                teacher_rep = teacher_rep[:, :min_len]\n",
    "            '''\n",
    "            #print(\"hidden states\")\n",
    "            #print(student_rep.shape,teacher_rep.shape)\n",
    "            kd_loss += loss_fn(student_rep, teacher_rep)\n",
    "\n",
    "    return kd_loss\n",
    "\n",
    "class DistillSTFTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_hidden_states_loss(self, student_hidden_states, teacher_hidden_states,loss_fn):\n",
    "        if teacher_hidden_states is None:\n",
    "            return 0.0\n",
    "        # Align teacher and student hidden states\n",
    "        teacher_hidden_states = get_cor_teacher(teacher_hidden_states, student_hidden_states, is_attn=False)\n",
    "        return get_kd_loss(student_hidden_states, teacher_hidden_states, loss_fn, is_attn=False, is_img=False)\n",
    "\n",
    "    def compute_attentions_loss(self, student_attentions, teacher_attentions,loss_fn):\n",
    "        if teacher_attentions is None:\n",
    "            return 0.0\n",
    "        # Align teacher and student attentions\n",
    "        teacher_attentions = get_cor_teacher(teacher_attentions, student_attentions, is_attn=True)\n",
    "        return get_kd_loss(student_attentions, teacher_attentions, loss_fn, is_attn=True, is_img=False)\n",
    "\n",
    "    def compute_image_hidden_states_loss(self, student_image_hidden_states, teacher_image_hidden_states,loss_fn):\n",
    "        if teacher_image_hidden_states is None or student_image_hidden_states is None:\n",
    "            return 0.0\n",
    "        # Vision towers are identical, no layer alignment needed\n",
    "        return get_kd_loss(student_image_hidden_states, teacher_image_hidden_states, loss_fn, is_attn=False, is_img=True)\n",
    "\n",
    "    @overrides()\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
    "        # Pop the teacher's outputs from the inputs\n",
    "        teacher_hidden_states = inputs.pop(\"teacher_hidden_states\", None)\n",
    "        teacher_attentions = inputs.pop(\"teacher_attentions\", None)\n",
    "        teacher_image_hidden_states = inputs.pop(\"teacher_image_hidden_states\", None)\n",
    "\n",
    "\n",
    "        # Compute the original loss from SFTTrainer\n",
    "        loss, outputs = super().compute_loss(model, inputs, return_outputs=True,num_items_in_batch=num_items_in_batch)\n",
    "\n",
    "        # Get student's internal states\n",
    "        student_hidden_states = outputs.hidden_states\n",
    "        student_attentions = outputs.attentions\n",
    "        student_image_hidden_states = outputs.image_hidden_states\n",
    "\n",
    "        # Compute the distillation loss\n",
    "        mse_loss = MSELoss()\n",
    "        hidden_states_loss = self.compute_hidden_states_loss(student_hidden_states, teacher_hidden_states, mse_loss)\n",
    "        attentions_loss = self.compute_attentions_loss(student_attentions, teacher_attentions, mse_loss)\n",
    "        image_hidden_states_loss = self.compute_image_hidden_states_loss(student_image_hidden_states, teacher_image_hidden_states, mse_loss)\n",
    "        print(hidden_states_loss,attentions_loss,image_hidden_states_loss)\n",
    "        # Combine the losses (example: simple addition, could be weighted)\n",
    "        distill_loss = hidden_states_loss + attentions_loss + image_hidden_states_loss\n",
    "\n",
    "        # You can weigh the original loss and the distillation loss\n",
    "        # Example: loss = 0.4 * loss + 0.6 * distill_loss\n",
    "        loss += distill_loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ],
   "id": "f3b1a2678c75f6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:42:19.039330Z",
     "start_time": "2025-08-22T07:42:19.032714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# This collate_fn is designed to handle the output from our new distill_dataset\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    teacher_hidden_states_list = []\n",
    "    teacher_attentions_list = []\n",
    "    teacher_image_hidden_states_list = []\n",
    "\n",
    "    # 1. Extract data from each sample\n",
    "    for example in examples:\n",
    "        # Standard processing for text and images\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "        # Extract teacher outputs, ensuring they are not None\n",
    "        if \"teacher_hidden_states\" in example and example[\"teacher_hidden_states\"] is not None:\n",
    "            teacher_hidden_states_list.append(example[\"teacher_hidden_states\"])\n",
    "        if \"teacher_attentions\" in example and example[\"teacher_attentions\"] is not None:\n",
    "            teacher_attentions_list.append(example[\"teacher_attentions\"])\n",
    "        if \"teacher_image_hidden_states\" in example and example[\"teacher_image_hidden_states\"] is not None:\n",
    "            teacher_image_hidden_states_list.append(example[\"teacher_image_hidden_states\"])\n",
    "\n",
    "    # 2. Process and tokenize text and images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 3. Create labels, masking where necessary\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # 4. Pad and stack the teacher's outputs\n",
    "    if teacher_hidden_states_list:\n",
    "        padded_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_hidden_states_list):\n",
    "            # pad_sequence expects a list of tensors\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_hidden_states\"] = tuple(padded_hidden_states)\n",
    "\n",
    "    if teacher_attentions_list:\n",
    "        padded_attentions = []\n",
    "        for layer_tensors in zip(*teacher_attentions_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_attentions.append(padded_layer)\n",
    "        batch[\"teacher_attentions\"] = tuple(padded_attentions)\n",
    "\n",
    "    if teacher_image_hidden_states_list:\n",
    "        padded_image_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_image_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_image_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_image_hidden_states\"] = tuple(padded_image_hidden_states)\n",
    "\n",
    "    return batch\n"
   ],
   "id": "e1e7a028",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:42:20.898216Z",
     "start_time": "2025-08-22T07:42:20.579404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"model/gemma3-distill\",     # directory to save and repository id\n",
    "    num_train_epochs=1,                         # number of training epochs\n",
    "    per_device_train_batch_size=1,              # batch size per device during training\n",
    "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=5,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    ")\n",
    "args.remove_unused_columns = False # important for collator\n",
    "\n",
    "# This collate_fn is designed to handle the output from our new distill_dataset\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    teacher_hidden_states_list = []\n",
    "    teacher_attentions_list = []\n",
    "    teacher_image_hidden_states_list = []\n",
    "\n",
    "    # 1. Extract data from each sample\n",
    "    for example in examples:\n",
    "        # Standard processing for text and images\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "        # Extract teacher outputs, ensuring they are not None\n",
    "        if \"teacher_hidden_states\" in example and example[\"teacher_hidden_states\"] is not None:\n",
    "            teacher_hidden_states_list.append(example[\"teacher_hidden_states\"])\n",
    "        if \"teacher_attentions\" in example and example[\"teacher_attentions\"] is not None:\n",
    "            teacher_attentions_list.append(example[\"teacher_attentions\"])\n",
    "        if \"teacher_image_hidden_states\" in example and example[\"teacher_image_hidden_states\"] is not None:\n",
    "            teacher_image_hidden_states_list.append(example[\"teacher_image_hidden_states\"])\n",
    "\n",
    "    # 2. Process and tokenize text and images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 3. Create labels, masking where necessary\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # 4. Pad and stack the teacher's outputs\n",
    "    if teacher_hidden_states_list:\n",
    "        padded_hidden_states = []\n",
    "        # Transpose and pad each layer\n",
    "        for layer_tensors in zip(*teacher_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_hidden_states\"] = tuple(padded_hidden_states)\n",
    "\n",
    "    if teacher_attentions_list:\n",
    "        padded_attentions = []\n",
    "        for layer_tensors in zip(*teacher_attentions_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_attentions.append(padded_layer)\n",
    "        batch[\"teacher_attentions\"] = tuple(padded_attentions)\n",
    "\n",
    "    if teacher_image_hidden_states_list:\n",
    "        padded_image_hidden_states = []\n",
    "        for layer_tensors in zip(*teacher_image_hidden_states_list):\n",
    "            padded_layer = pad_sequence(list(layer_tensors), batch_first=True, padding_value=0.0)\n",
    "            padded_image_hidden_states.append(padded_layer)\n",
    "        batch[\"teacher_image_hidden_states\"] = tuple(padded_image_hidden_states)\n",
    "\n",
    "    return batch"
   ],
   "id": "98e2c110cf65d720",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtrl\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SFTConfig\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pad_sequence\n\u001B[0;32m----> 4\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mSFTConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel/gemma3-distill\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;66;43;03m# directory to save and repository id\u001B[39;49;00m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                         \u001B[49m\u001B[38;5;66;43;03m# number of training epochs\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m              \u001B[49m\u001B[38;5;66;43;03m# batch size per device during training\u001B[39;49;00m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m              \u001B[49m\u001B[38;5;66;43;03m# number of steps before performing a backward/update pass\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_checkpointing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# use gradient checkpointing to save memory\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madamw_torch_fused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                  \u001B[49m\u001B[38;5;66;43;03m# use fused adamw optimizer\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                            \u001B[49m\u001B[38;5;66;43;03m# log every 5 steps\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                      \u001B[49m\u001B[38;5;66;43;03m# save checkpoint every epoch\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                         \u001B[49m\u001B[38;5;66;43;03m# learning rate, based on QLoRA paper\u001B[39;49;00m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbf16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m                                  \u001B[49m\u001B[38;5;66;43;03m# use bfloat16 precision\u001B[39;49;00m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                          \u001B[49m\u001B[38;5;66;43;03m# max gradient norm based on QLoRA paper\u001B[39;49;00m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarmup_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.03\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                          \u001B[49m\u001B[38;5;66;43;03m# warmup ratio based on QLoRA paper\u001B[39;49;00m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr_scheduler_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconstant\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m               \u001B[49m\u001B[38;5;66;43;03m# use constant learning rate scheduler\u001B[39;49;00m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtensorboard\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                    \u001B[49m\u001B[38;5;66;43;03m# report metrics to tensorboard\u001B[39;49;00m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_checkpointing_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muse_reentrant\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# use reentrant checkpointing\u001B[39;49;00m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                      \u001B[49m\u001B[38;5;66;43;03m# need a dummy field for collator\u001B[39;49;00m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mskip_prepare_dataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# important for collator\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m args\u001B[38;5;241m.\u001B[39mremove_unused_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;66;03m# important for collator\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# This collate_fn is designed to handle the output from our new distill_dataset\u001B[39;00m\n",
      "File \u001B[0;32m<string>:149\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, model_init_kwargs, chat_template_path, dataset_text_field, dataset_kwargs, dataset_num_proc, eos_token, pad_token, max_length, packing, packing_strategy, padding_free, pad_to_multiple_of, eval_packing, completion_only_loss, assistant_only_loss, activation_offloading)\u001B[0m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_config.py:247\u001B[0m, in \u001B[0;36mSFTConfig.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__post_init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__post_init__\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1729\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1727\u001B[0m                     error_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m You need Ampere+ GPU with cuda>=11.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1728\u001B[0m                 \u001B[38;5;66;03m# gpu\u001B[39;00m\n\u001B[0;32m-> 1729\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_message)\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16:\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt most one of fp16 and bf16 can be True, but not both\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Your setup doesn't support bf16/gpu."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## student model",
   "id": "39db91883cba8018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:39:40.484089Z",
     "start_time": "2025-08-22T07:39:39.635032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, Gemma3ForConditionalGeneration,BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "# 1) 載入「原始 Gemma」(不要量化)\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU，安裝flash attn前必須先安裝ninja，詳細:https://blog.csdn.net/lckj2009/article/details/136054392\n",
    "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"cuda:0\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# 1) 載入「原始 Gemma」(不要量化)\n",
    "base_model_clean = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# 2) 建立半層數設定\n",
    "student_config = copy.deepcopy(base_model_clean.config)\n",
    "orig_layers = student_config.text_config.num_hidden_layers\n",
    "half_layers = orig_layers // 2\n",
    "student_config.text_config.num_hidden_layers = half_layers\n",
    "print(f\"Original decoder layers: {orig_layers} -> Student: {half_layers}\")\n",
    "\n",
    "# Explicitly enable outputting hidden states and attentions for the student model\n",
    "student_config.output_hidden_states = True\n",
    "student_config.output_attentions = True\n",
    "if hasattr(student_config, \"vision_config\"):\n",
    "    student_config.vision_config.output_hidden_states = True\n",
    "\n",
    "# 3) 建立學生模型（非量化）\n",
    "student_model = Gemma3ForConditionalGeneration(config=student_config).to(\n",
    "    base_model_clean.device, dtype=base_model_clean.dtype\n",
    ")\n",
    "\n",
    "# 4) 複製非層級權重\n",
    "with torch.no_grad():\n",
    "    # 視覺塔與投影器\n",
    "    student_model.model.vision_tower.load_state_dict(base_model_clean.model.vision_tower.state_dict())\n",
    "    student_model.model.multi_modal_projector.load_state_dict(base_model_clean.model.multi_modal_projector.state_dict())\n",
    "    # 文本嵌入與最終層正規化\n",
    "    student_model.model.language_model.embed_tokens.load_state_dict(\n",
    "        base_model_clean.model.language_model.embed_tokens.state_dict()\n",
    "    )\n",
    "    student_model.model.language_model.norm.load_state_dict(\n",
    "        base_model_clean.model.language_model.norm.state_dict()\n",
    "    )\n",
    "    # LM Head\n",
    "    student_model.lm_head.load_state_dict(base_model_clean.lm_head.state_dict())\n",
    "\n",
    "# 5) 定義兩層合一層的權重合併（逐元素平均）\n",
    "def average_state_dicts(sd_a: dict, sd_b: dict, alpha: float = 0.5) -> dict:\n",
    "    merged = {}\n",
    "    keys = sd_a.keys() & sd_b.keys()\n",
    "    for k in keys:\n",
    "        ta, tb = sd_a[k], sd_b[k]\n",
    "        if isinstance(ta, torch.Tensor) and isinstance(tb, torch.Tensor) and ta.shape == tb.shape:\n",
    "            # 只對浮點張量做平均，其餘沿用第一個\n",
    "            if torch.is_floating_point(ta) and torch.is_floating_point(tb):\n",
    "                merged[k] = (1 - alpha) * ta + alpha * tb\n",
    "            else:\n",
    "                merged[k] = ta\n",
    "        else:\n",
    "            merged[k] = ta\n",
    "    # 帶入 sd_a 獨有的鍵\n",
    "    for k in sd_a.keys() - keys:\n",
    "        merged[k] = sd_a[k]\n",
    "    return merged\n",
    "\n",
    "# 6) 以「每 2 層合成 1 層」方式拷貝到學生層\n",
    "with torch.no_grad():\n",
    "    for i in range(half_layers):\n",
    "        t_layer_a = base_model_clean.model.language_model.layers[2 * i]\n",
    "        t_layer_b = base_model_clean.model.language_model.layers[2 * i + 1]\n",
    "        s_layer = student_model.model.language_model.layers[i]\n",
    "\n",
    "        merged_sd = average_state_dicts(\n",
    "            t_layer_a.state_dict(),\n",
    "            t_layer_b.state_dict(),\n",
    "            alpha=1,  # 可調整權重比例\n",
    "        )\n",
    "        s_layer.load_state_dict(merged_sd, strict=False)\n",
    "with torch.no_grad():\n",
    "    student_model.lm_head.weight.data = student_model.model.language_model.embed_tokens.weight.data\n",
    "\n",
    "print(\"Student model built with 2-teacher-layers -> 1-student-layer merging.\")"
   ],
   "id": "7f25ed6788d75bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 23.56 GiB of which 144.94 MiB is free. Process 2265815 has 22.73 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 79.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 16\u001B[0m\n\u001B[1;32m      9\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m     10\u001B[0m     attn_implementation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# Use \"flash_attention_2\" when running on Ampere or newer GPU，安裝flash attn前必須先安裝ninja，詳細:https://blog.csdn.net/lckj2009/article/details/136054392\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16, \u001B[38;5;66;03m# What torch dtype to use, defaults to auto\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# Let torch decide how to load the model\u001B[39;00m\n\u001B[1;32m     13\u001B[0m )\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 1) 載入「原始 Gemma」(不要量化)\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m base_model_clean \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForImageTextToText\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# 2) 建立半層數設定\u001B[39;00m\n\u001B[1;32m     22\u001B[0m student_config \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(base_model_clean\u001B[38;5;241m.\u001B[39mconfig)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:600\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    599\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[0;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    604\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    605\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    606\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:317\u001B[0m, in \u001B[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    315\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5074\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   5064\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5065\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   5067\u001B[0m     (\n\u001B[1;32m   5068\u001B[0m         model,\n\u001B[1;32m   5069\u001B[0m         missing_keys,\n\u001B[1;32m   5070\u001B[0m         unexpected_keys,\n\u001B[1;32m   5071\u001B[0m         mismatched_keys,\n\u001B[1;32m   5072\u001B[0m         offload_index,\n\u001B[1;32m   5073\u001B[0m         error_msgs,\n\u001B[0;32m-> 5074\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5075\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5076\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5077\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5078\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5079\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5080\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5081\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5082\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5083\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5084\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5085\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5086\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5087\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5088\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5089\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5090\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5091\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   5092\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5537\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[1;32m   5534\u001B[0m         args_list \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mtqdm(args_list, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading checkpoint shards\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   5536\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[0;32m-> 5537\u001B[0m         _error_msgs, disk_offload_index, cpu_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5538\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m _error_msgs\n\u001B[1;32m   5540\u001B[0m \u001B[38;5;66;03m# Adjust offloaded weights name and save if needed\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:975\u001B[0m, in \u001B[0;36mload_shard_file\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[0;32m--> 975\u001B[0m     disk_offload_index, cpu_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcpu_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    987\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_safetensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_offloaded_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[43m        \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    990\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    991\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:843\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001B[0m\n\u001B[1;32m    833\u001B[0m         hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(\n\u001B[1;32m    834\u001B[0m             model,\n\u001B[1;32m    835\u001B[0m             param,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    840\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msharding_kwargs,\n\u001B[1;32m    841\u001B[0m         )\n\u001B[1;32m    842\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 843\u001B[0m     param \u001B[38;5;241m=\u001B[39m \u001B[43mparam\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    844\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m casting_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    845\u001B[0m         param \u001B[38;5;241m=\u001B[39m param\u001B[38;5;241m.\u001B[39mto(casting_dtype)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 23.56 GiB of which 144.94 MiB is free. Process 2265815 has 22.73 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 79.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:39:03.384648Z",
     "start_time": "2025-08-22T07:39:03.375977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del base_model_clean\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "bc9d3a564b1b8c84",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "e8958ad6676e48df",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5222ee33ee67c",
   "metadata": {},
   "source": [
    "## 讀回蒸餾資料"
   ]
  },
  {
   "cell_type": "code",
   "id": "f124725cd2ddf69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T13:56:15.459762Z",
     "start_time": "2025-08-19T13:55:46.714905Z"
    }
   },
   "source": [
    "import pandas as pd, io, base64, numpy as np, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def deserialize_tensor_list(b64str):\n",
    "    if not isinstance(b64str, str) or b64str == \"\":\n",
    "        return None\n",
    "    data = base64.b64decode(b64str.encode(\"ascii\"))\n",
    "    buf = io.BytesIO(data)\n",
    "    with np.load(buf, allow_pickle=False) as npz:\n",
    "        keys = sorted(npz.files, key=lambda k: int(k.split(\"_\")[1]))\n",
    "        tensors = [torch.from_numpy(npz[k]).to(torch.float32) for k in keys]\n",
    "    return tensors\n",
    "\n",
    "class CSVTeacherSignalsDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, raw_dataset):\n",
    "        self.df = pd.read_csv(csv_path, compression=\"infer\")\n",
    "        self.raw_dataset = raw_dataset\n",
    "        assert self.df[\"id\"].max() < len(raw_dataset), \"CSV ids exceed raw dataset length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        raw = self.raw_dataset[int(row[\"id\"])]\n",
    "        return {\n",
    "            \"messages\": raw[\"messages\"],\n",
    "            \"teacher_hidden_states\": deserialize_tensor_list(row.get(\"teacher_hidden_states_b64\", \"\")),\n",
    "            \"teacher_attentions\": deserialize_tensor_list(row.get(\"teacher_attentions_b64\", \"\")),\n",
    "            \"teacher_image_hidden_states\": deserialize_tensor_list(row.get(\"teacher_image_hidden_states_b64\", \"\")),\n",
    "        }\n",
    "\n",
    "distill_dataset = CSVTeacherSignalsDataset(\"dataset/distill_teacher_signals.csv\", dataset)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f0065fd1b9470f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T13:56:19.679635Z",
     "start_time": "2025-08-19T13:56:15.545054Z"
    }
   },
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "7b442fb4f03fb09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:47:29.869761Z",
     "start_time": "2025-08-19T14:47:29.030735Z"
    }
   },
   "source": [
    "trainer = DistillSTFTrainer(\n",
    "    model=student_model,\n",
    "    args=args,\n",
    "    train_dataset=distill_dataset,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "8a437d39f2468694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:47:43.760039Z",
     "start_time": "2025-08-19T14:47:30.472998Z"
    }
   },
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model(safe_serialization=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 405, 2560]) torch.Size([1, 405, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 458, 2560]) torch.Size([1, 458, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 404, 2560]) torch.Size([1, 404, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/13 : < :, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 409, 2560]) torch.Size([1, 409, 2560])\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>) tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 70.94 MiB is free. Process 926453 has 23.13 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start training, the model will be automatically saved to the Hub and the output directory\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Save the final model again to the Hugging Face Hub\u001B[39;00m\n\u001B[1;32m      5\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model(safe_serialization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2238\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2236\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2238\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2239\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2243\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2582\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2575\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2576\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2577\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2578\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2579\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2580\u001B[0m )\n\u001B[1;32m   2581\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2582\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2585\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2586\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2587\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2588\u001B[0m ):\n\u001B[1;32m   2589\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2590\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:904\u001B[0m, in \u001B[0;36mSFTTrainer.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaybe_activation_offload_context:\n\u001B[0;32m--> 904\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3845\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3842\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED:\n\u001B[1;32m   3843\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_wrt_gas\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 3845\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3847\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2734\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2733\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2734\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1124\u001B[0m, in \u001B[0;36m_checkpoint_hook.__init__.<locals>.unpack_hook\u001B[0;34m(holder)\u001B[0m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _recomputation_hook(\n\u001B[1;32m   1122\u001B[0m         weakref\u001B[38;5;241m.\u001B[39mref(frame), gid\n\u001B[1;32m   1123\u001B[0m     ), torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m-> 1124\u001B[0m         \u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecompute_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _StopRecomputationError:\n\u001B[1;32m   1126\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1518\u001B[0m, in \u001B[0;36m_checkpoint_without_reentrant_generator.<locals>.recompute_fn\u001B[0;34m(*inputs)\u001B[0m\n\u001B[1;32m   1514\u001B[0m device_autocast_ctx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast(\n\u001B[1;32m   1515\u001B[0m     device_type\u001B[38;5;241m=\u001B[39mdevice_type, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_autocast_kwargs\n\u001B[1;32m   1516\u001B[0m ) \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mis_autocast_available(device_type) \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext()\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m device_autocast_ctx, torch\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcpu_autocast_kwargs), recompute_context:  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1518\u001B[0m     \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:450\u001B[0m, in \u001B[0;36mSiglipEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    447\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    449\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm1(hidden_states)\n\u001B[0;32m--> 450\u001B[0m hidden_states, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    457\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:389\u001B[0m, in \u001B[0;36mSiglipAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    387\u001B[0m     attention_interface \u001B[38;5;241m=\u001B[39m ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation]\n\u001B[0;32m--> 389\u001B[0m attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    400\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(batch_size, seq_length, embed_dim)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    401\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj(attn_output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py:335\u001B[0m, in \u001B[0;36meager_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    333\u001B[0m     attn_weights \u001B[38;5;241m=\u001B[39m attn_weights \u001B[38;5;241m+\u001B[39m attention_mask\n\u001B[0;32m--> 335\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(query\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    336\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(attn_weights, p\u001B[38;5;241m=\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39mmodule\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    338\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(attn_weights, value)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2142\u001B[0m, in \u001B[0;36msoftmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   2140\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(dim)\n\u001B[1;32m   2141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2142\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 70.94 MiB is free. Process 926453 has 23.13 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0aae556ebfc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
