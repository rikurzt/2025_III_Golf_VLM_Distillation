{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 準備資料",
   "id": "598008aca91c29d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8abe9ca9ad505d45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:30:36.141539Z",
     "start_time": "2025-06-18T11:30:36.137713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n"
   ],
   "id": "fd85008faf8831fb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:37.372043Z",
     "start_time": "2025-04-18T09:04:37.177231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_locate = \"/tmp/pycharm_project_159/\" #遠端環境\n",
    "dataset_locate = file_locate+\"/dataset/TA_model_FineTune_dataset/\" \n",
    "images = os.listdir(file_locate + \"/dataset/TA_model_FineTune_dataset/images\")\n",
    "Inputs = pd.read_csv(file_locate + \"/dataset/TA_model_FineTune_dataset/question_Input.csv\")\n",
    "\n",
    "init_prompt = open(file_locate + \"/dataset/init_prompt.txt\").read()\n",
    "rule = pd.read_excel(file_locate + \"/dataset/回饋規則.xlsx\")\n",
    "\n",
    "#combine images and Inputs\n",
    "def combine_images_and_inputs(images, Inputs):\n",
    "    combined_data = []\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        image_path = os.path.join(file_locate + \"/dataset/TA_model_FineTune_dataset/images/\", images[i])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_text = Inputs.iloc[i][\"Input\"]\n",
    "        response = Inputs.iloc[i][\"GroundTruth\"]\n",
    "        combined_data.append({\"image\": image, \"input\": input_text,\"response\":response} )\n",
    "\n",
    "    return combined_data\n",
    "dataset = combine_images_and_inputs(images, Inputs)"
   ],
   "id": "1f9c3b71e3c05397",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:37.435363Z",
     "start_time": "2025-04-18T09:04:37.432164Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "61f151d2767afee5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Pull Hook 左拉左曲球\\n球速:66.43、發射角度:7.198、發射方向:-6.172、飛行距離:179、ClubAngleFace:-15、ClubAnglePath:6.901\\nfront:{A: [0.0, 0.0, -0.0, 0.0, -0.0, -0.0], F: [-0.01, -0.008, 0.0, 0.048, -0.018, -0.011], I: [-0.019, -0.04, 0.184, -0.063, -0.041, -0.023], T: [0.002, 0.0, 0.0, -0.001, -0.001, -0.001]}\\nside:{A: [0.002, -0.001, 0.001, -0.0, -0.002, 0.0], F: [0.001, 0.004, 0.0, -0.001, -0.003, 0.0], I: [0.217, -0.065, -0.013, -0.068, -0.054, -0.013], T: [-0.049, -0.043, 0.253, -0.083, -0.055, -0.013]}',\n",
       "  'response': '        \"球路\": \"Pull Hook 左拉左曲球\",\\n        \"原因\": \"下桿角度過於陡峭，手腕過度彎曲，擊球時桿面過度關閉\",\\n        \"建議\": \"在下桿（P5-P6）時，試著讓右肩與右手肘保持外旋，避免過度手腕翻轉，並確保手腕在P5.5時維持適當的屈曲角度，這有助於讓球桿的路徑更穩定。此外，擊球時（P7），確保左手腕恢復至較中立的狀態，以防止桿面過度關閉，造成左拉左曲球。\"  '},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Fade 小右曲球\\n球速:70.33、發射角度:9.56、發射方向:1.486、飛行距離:123.7、ClubAngleFace:2.66、ClubAnglePath:0.192\\nfront:{A: [-0.0, 0.0, 0.0, -0.0, -0.0, -0.0], F: [-0.006, -0.005, -0.0, 0.047, -0.022, -0.014], I: [0.0, -0.0, -0.0, -0.0, 0.001, -0.0], T: [0.042, -0.008, 0.0, -0.018, -0.011, -0.006]}\\nside:{A: [0.001, -0.001, 0.0, -0.0, -0.001, 0.0], F: [-0.0, 0.0, 0.0, -0.002, -0.002, 0.003], I: [-0.005, -0.011, -0.002, 0.028, -0.009, -0.001], T: [0.001, -0.0, -0.0, -0.001, 0.0, 0.0]}',\n",
       "  'response': '        \"球路\": \"Fade 小右曲球\",\\n        \"原因\": \"手腕角度保持得很好，桿面觸球方正，且揮桿軌跡由外向內所致。\",\\n        \"建議\": \"這次上杆高度適中且動作流暢，下杆時也保持穩定。教練指出這是由於下半身主導力量，使擊球姿勢自然且穩定，展現出漂亮的旋轉動作與擊球姿態，充分利用身體的力量而非過度依賴手腕。您可以在P4至P5的轉換階段時，確保重心平穩轉移，避免過度依靠手臂拉動球桿，使擊球更加紮實。請繼續保持這種流暢的技巧，並強化身體與揮桿動作的連結，以穩定Fade球路。\"'},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Push Slice 右拉右曲球\\n球速:61.868、發射角度:17.004、發射方向:1.799、飛行距離:113.4、ClubAngleFace:3.43、ClubAnglePath:-4.322\\nfront:{A: [-0.0, 0.0, -0.0, -0.0, 0.0, -0.0], F: [-0.002, -0.009, 0.001, 0.044, -0.021, -0.013], I: [-0.0, -0.0, -0.0, -0.0, 0.001, -0.0], T: [0.042, -0.008, 0.0, -0.018, -0.011, -0.006]}\\nside:{A: [0.013, -0.003, 0.002, -0.005, -0.005, -0.001], F: [0.002, 0.003, 0.0, 0.0, -0.005, 0.0], I: [0.016, -0.019, 0.003, 0.017, -0.016, -0.002], T: [0.005, 0.002, -0.002, -0.003, -0.002, 0.0]}',\n",
       "  'response': '    \"球路\": \"Push Slice 右拉右曲球\",\\n    \"原因\": \"P7擊球時桿面開放，擊球點位於球的內側，左手腕未固定\",\\n    \"建議\": \"擊球時，左手腕維持固定，注意擊球點位置。當下桿進入P6至P7時，專注於讓左手腕呈現較穩定的角度，不要讓手腕過度外展，這樣能確保桿面更平方地進入擊球點。此外，練習透過身體帶動揮桿，而不是單靠手腕調整球桿，使擊球點更接近球的中心，這將有助於減少右曲球問題。\"'},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Pull Hook 左拉左曲球\\n球速:88.875、發射角度:7.848、發射方向:-7.51、飛行距離:274.3、ClubAngleFace:-15、ClubAnglePath:3.91\\nfront:{A: [-0.024, -0.052, -0.016, 0.172, -0.052, -0.026], F: [0.0, 0.002, 0.001, -0.001, -0.001, -0.001], I: [0.0, 0.002, 0.0, -0.001, -0.001, -0.001], T: [0.013, 0.002, -0.0, -0.007, -0.005, -0.003]}\\nside:{A: [0.001, 0.001, 0.001, 0.0, -0.002, -0.0], F: [0.051, -0.009, 0.052, -0.034, -0.097, 0.031], I: [-0.042, -0.067, -0.034, 0.225, -0.055, -0.017], T: [0.005, 0.003, -0.002, -0.004, -0.002, 0.0]}',\n",
       "  'response': '        \"球路\": \"Pull Hook 左拉左曲球\",\\n        \"原因\": \"下桿時桿面過於關閉，擊球點位於球的外側，桿面相對於桿頭路徑過於向左\",\\n        \"建議\": \"在下桿(P5~P6)時，請專注於手腕控制，確保左手腕不過度屈曲，並保持桿面與球道的對齊。可以在練習時嘗試在P5.5時讓手腕維持較中性的角度，避免擊球時桿面封閉過多。此外，確保下桿時不過度從內側揮擊，試著讓球桿沿著目標線方向更自然下桿，這將幫助你的球飛行更加穩定。\"'},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Push Slice 右拉右曲球\\n球速:100.069、發射角度:17.951、發射方向:6.794、飛行距離:406.9、ClubAngleFace:7.36、ClubAnglePath:4.696\\nfront:{A: [0.145, -0.045, -0.006, -0.048, -0.035, -0.013], F: [-0.001, 0.004, 0.004, -0.004, -0.004, 0.001], I: [0.04, -0.023, -0.002, 0.013, -0.016, -0.008], T: [0.004, 0.003, -0.002, -0.003, -0.002, -0.001]}\\nside:{A: [0.186, -0.091, 0.158, -0.121, -0.086, -0.048], F: [-0.0, 0.004, 0.003, -0.003, -0.003, -0.001], I: [-0.023, -0.052, -0.019, -0.072, -0.051, 0.22], T: [0.02, -0.012, 0.052, -0.031, -0.021, -0.012]}',\n",
       "  'response': '        \"球路\": \"Push Slice右拉右曲球\",\\n        \"原因\": \"P5~6下桿角度過於平緩，左手腕過度外展，肩關節伸展抬起，腰部向前旋轉過度，身體重心太早向前移動\",\\n        \"建議\": \"下桿時，手腕及肩膀往左轉動，腰部減少旋轉\"'},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=3040x2160>,\n",
       "  'input': 'Push Slice 右拉右曲球\\n球速:72.111、發射角度:12.173、發射方向:6.402、飛行距離:168.8、ClubAngleFace:8.779、ClubAnglePath:-2.799\\nfront:{A: [0.177, -0.089, 0.163, -0.122, -0.088, -0.044], F: [-0.009, -0.011, 0.0, 0.049, -0.018, -0.012], I: [-0.02, -0.04, 0.182, -0.062, -0.04, -0.023], T: [0.004, -0.001, 0.0, -0.002, -0.001, -0.001]}\\nside:{A: [0.0, -0.002, 0.005, -0.002, -0.002, -0.0], F: [0.001, -0.001, 0.0, -0.002, -0.001, 0.002], I: [0.055, -0.026, 0.008, -0.007, -0.019, -0.009], T: [0.0, -0.0, 0.0, -0.0, -0.0, 0.0]}',\n",
       "  'response': '  \"球路\": \"Push Slice 右拉右曲球\",\\n  \"原因\": \"P5~6下桿角度過於平緩，左手腕過度外展，肩關節伸展抬起，腰部向前旋轉過度，身體重心太早向前移動\",\\n  \"建議\": \"在P5至P6的下桿階段，建議您專注於讓手腕保持穩定，避免過度外展，同時確保肩膀不會抬升過高。您可以試著在下桿時，集中於讓右手肘內旋，並讓腰部的轉動保持在適當的範圍內，避免過度旋轉導致重心失衡。此外，下桿時可稍微加強從下半身啟動的感覺，讓身體的動作更加連貫，減少桿面過於開放的情況。這樣能夠有效改善球的初始方向，減少過度右曲的球路。\"'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:37.495764Z",
     "start_time": "2025-04-18T09:04:37.492983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# System message for the assistant\n",
    "system_message = \"You are an expert product description writer for Amazon.\"\n",
    "\n",
    "# User prompt that combines the user query and the schema\n",
    "user_prompt = \"\"\"Create a Short Product description based on the provided <PRODUCT> and <CATEGORY> and image.\n",
    "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
    "\n",
    "<PRODUCT>\n",
    "{product}\n",
    "</PRODUCT>\n",
    "\n",
    "<CATEGORY>\n",
    "{category}\n",
    "</CATEGORY>\n",
    "\"\"\"\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": init_prompt}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": sample[\"input\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"response\"]}],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    image_inputs = []\n",
    "    # Iterate through each conversation\n",
    "    for msg in messages:\n",
    "        # Get content (ensure it's a list)\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        # Check each content element for images\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # Get the image and convert to RGB\n",
    "                if \"image\" in element:\n",
    "                    image = element[\"image\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                image_inputs.append(image.convert(\"RGB\"))\n",
    "    return image_inputs\n",
    "\n"
   ],
   "id": "d141cedcefd8119f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T10:18:27.140897Z",
     "start_time": "2025-06-19T10:18:27.003494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Convert dataset to OAI messages\n",
    "# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[0][\"messages\"][0])"
   ],
   "id": "4602a30a9487f76d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Convert dataset to OAI messages\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m [format_data(sample) \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdataset\u001B[49m]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6852a0a49dcda2a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:38.394270Z",
     "start_time": "2025-04-18T09:04:37.618014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig"
   ],
   "id": "bcc40ce4d3fadfa4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rikurzt/.virtualenvs/TESTSSH/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:46.212882Z",
     "start_time": "2025-04-18T09:04:38.411048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-4b-pt\" \n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\",use_fast=True)"
   ],
   "id": "c59b3416c51dba9d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:46.288206Z",
     "start_time": "2025-04-18T09:04:46.234984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-product-description\",     # directory to save and repository id\n",
    "    num_train_epochs=1,                         # number of training epochs\n",
    "    per_device_train_batch_size=1,              # batch size per device during training\n",
    "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=1,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    push_to_hub=False,                           # push model to hub\n",
    "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    ")\n",
    "args.remove_unused_columns = False # important for collator\n",
    "\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        \n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ],
   "id": "7ce97bc2cb68a857",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:46.345440Z",
     "start_time": "2025-04-18T09:04:46.310006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ],
   "id": "43badd939b5cbbb2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:04:46.949829Z",
     "start_time": "2025-04-18T09:04:46.364998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ],
   "id": "d6a6e4b490557492",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:05:08.891435Z",
     "start_time": "2025-04-18T09:04:46.975459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ],
   "id": "14eee810eb2f6bcc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.615300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:05:08.984305Z",
     "start_time": "2025-04-18T09:05:08.903734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "4e2933c7b4b3f960",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:05:12.494050Z",
     "start_time": "2025-04-18T09:05:08.996007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "  args.output_dir,\n",
    "  device_map=\"cuda:0\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  attn_implementation=\"eager\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)"
   ],
   "id": "41a790a05fa92fac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:05:12.768253Z",
     "start_time": "2025-04-18T09:05:12.508988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig.from_pretrained(\"google/gemma-3-4b-pt\")\n",
    "generation_config.cache_implementation = \"dynamic\""
   ],
   "id": "7f2a9541dbe34717",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:12:00.821845Z",
     "start_time": "2025-04-18T09:11:29.768879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_description(model, processor):\n",
    "    # Convert sample into messages and then apply the chat template\n",
    "    img = Image.open(dataset_locate+\"/images/combined_186382.jpg\").convert(\"RGB\")\n",
    "    messages = [\n",
    "            {\"role\": \"system\",\"content\": [{\"type\": \"text\", \"text\": init_prompt}],},\n",
    "            {\"role\": \"user\",\n",
    "              \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": dataset[0]['messages'][1]['content'][0]['text']},\n",
    "                    {\"type\": \"image\", \"image\": img},\n",
    "              ],\n",
    "            },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # Process the image and text\n",
    "    image_inputs = process_vision_info(messages)\n",
    "    # Tokenize the text and process the images\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Move the inputs to the device\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    stop_token_ids = [processor.tokenizer.eos_token_id, processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024, top_p=1.0, do_sample=True, temperature=0.3, eos_token_id=stop_token_ids, disable_compile=True)\n",
    "    # Trim the generation and decode the output to text\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# generate the description\n",
    "description = generate_description(model, processor)\n",
    "print(description)"
   ],
   "id": "8b4b619b3115a4b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一位充滿教學熱忱的高爾夫球教練，我會給你以下資訊:\n",
      "1.某位學員在P2上桿(A)、P3上桿(A)、P4 頂點/轉換(F)、P5~6 下桿(I)、P7 擊球(T)的高爾夫擊球動作片段圖集，總共十張，上半部五張是學員正面的照片，下半部五張是學員側面的圖片，每張圖片紅色點與綠色的線代表學員的骨架，白色輔助線代表球員所在位置 ，粉色的輔助線代表球員的集求姿態不該超越的位置，橙色的輔助線代表揮桿夾角\n",
      "2.學員的擊球數據，包含球速、發射角度、發射方向(球相對於目標線開始的初始方向。正發射方向表示球從目標右側開始，負發射方向表示球從目標左側開始)、飛行距離、ClubAngleFace(高爾夫球手將此稱為具有“開放”或“封閉”桿面。技術定義：在高爾夫球最大壓縮時，球桿面和高爾夫球接觸中心點的水平球桿面方向)、ClubAnglePath(高爾夫擊球的預期曲率(旋轉軸)的關鍵因素。假設中心接觸，球應該朝向面角彎曲並遠離球桿路徑(+5、0、-5))\n",
      "3.與教練的正面和側面姿勢在A、F、I、T四個階段各個關節的差異，格式為front:{{A:[肩膀,手肘,手腕,髖部,膝蓋,腳踝],F:[......}}side:{{A:[肩膀,手肘,手腕,髖部,膝蓋,腳踝],F:[......}}\n",
      "4.回饋規則\n",
      "5.Clubface = clubAnglePath - clubAngleFace，大於3代表在原本的球路上還會偏左，小於3代表在原本的球路上還會遍右，例如左飛球碰到Clubface 大於3就會變成左拉左曲球\n",
      "6.正確輸出建議模板:\"......在我們全面的揮杆分析中，您的揮杆動作與教練大致相同，請繼續保持......\"\n",
      "7.失誤輸出建議模板:\"......在我們全面的揮杆分析中，您的揮杆動作與教練最相似的地方在於<階段>，請繼續保持，然而您的揮杆有幾個地方可以加強。在您的<階段>、考慮調整您的<關節>，試著<調整方法>，這將有助於......\"\n",
      "\n",
      "\n",
      "\n",
      "你有以下幾項任務:\n",
      "1.觀察學員正面與側面圖片的骨架以及參考線，找到姿勢不正確的圖片\n",
      "2.判斷資訊3中與教練的姿勢差異是否有特別突出的地方，如果差異都不大，則判斷是那種正確球路，如果差異有特別突出的地方，則判斷是那種失誤球路\n",
      "3.每次分析都需要根據任務2從回饋規則內尋找最相似的原因，結合Clubface以及圖片判斷屬於回饋規則內的哪一種球路類型，並參考回饋規則內的結果是失誤還是正確，即使球路正確也要判斷是哪種球路，球路嚴重失誤則不需要再進行建議分析，不要只參照數值，請不要自創球路名稱\n",
      "4.結合任務1、2、3以及學員的擊球數據參數，根據學員的姿勢參考回饋規則內的建議範例以及輸出建議模板提供100字以上詳細的一到兩個階段的口語化動作細節具體來說如何調整的建議，並嘗試模仿真人教練的口吻，不要完全照抄範例\n",
      "5. 經過多種原因分析與判斷後，請統一球路類型並提供整合的分析建議為此格式:\n",
      "{{\n",
      "        \"球路\": \"球路類型\",\n",
      "         \"原因\": \"原因文字\",\n",
      "         \"建議\": \"建議文字\"}}\n",
      "6.不須描述非建議內容的輸出範例和執行步驟，所有文字都需要在格式內\n",
      "\n",
      "Pull Hook 左拉左曲球\n",
      "球速:66.43、發射角度:7.198、發射方向:-6.172、飛行距離:179、ClubAngleFace:-15、ClubAnglePath:6.901\n",
      "front:{A: [0.0, 0.0, -0.0, 0.0, -0.0, -0.0], F: [-0.01, -0.008\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:14:47.939221Z",
     "start_time": "2025-04-18T09:14:17.039886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_description(model, processor):\n",
    "    # Convert sample into messages and then apply the chat template\n",
    "    img = Image.open(dataset_locate+\"/images/combined_186382.jpg\").convert(\"RGB\")\n",
    "    messages = [\n",
    "            {\"role\": \"system\",\"content\": [{\"type\": \"text\", \"text\": \"\"}],},\n",
    "            {\"role\": \"user\",\n",
    "              \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"你看到什麼\"},\n",
    "                    {\"type\": \"image\", \"image\": img},\n",
    "              ],\n",
    "            },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # Process the image and text\n",
    "    image_inputs = process_vision_info(messages)\n",
    "    # Tokenize the text and process the images\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Move the inputs to the device\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    stop_token_ids = [processor.tokenizer.eos_token_id, processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024, top_p=1.0, do_sample=True, temperature=0.3, eos_token_id=stop_token_ids, disable_compile=True)\n",
    "    # Trim the generation and decode the output to text\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# generate the description\n",
    "description = generate_description(model, processor)\n",
    "print(description)"
   ],
   "id": "696d99292f3e5a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫木\n",
      "枫\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
